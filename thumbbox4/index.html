<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>探索OCR——在印刷体表格识别的尝试 - 布丁布瓜</title><meta name=description content="AI，游戏，摄影，随笔。有关理智与趣味的分享。"><meta property="og:title" content="探索OCR——在印刷体表格识别的尝试"><meta property="og:description" content="去年年底在做的一个研究课题，有关于OCR财务报表识别。初次探索这个在市面上有不少落地产品的新鲜领域，简单总结一下课题的成果。"><meta property="og:type" content="article"><meta property="og:url" content="https://shakex.github.io/thumbbox4/"><meta property="og:image" content="https://shakex.github.io/thumbbox4/featured-image.png"><meta property="article:published_time" content="2021-04-17T00:00:00+00:00"><meta property="article:modified_time" content="2021-04-17T00:00:00+00:00"><meta property="og:site_name" content="布丁布瓜"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shakex.github.io/thumbbox4/featured-image.png"><meta name=twitter:title content="探索OCR——在印刷体表格识别的尝试"><meta name=twitter:description content="去年年底在做的一个研究课题，有关于OCR财务报表识别。初次探索这个在市面上有不少落地产品的新鲜领域，简单总结一下课题的成果。"><meta name=application-name content="CodeIT"><meta name=apple-mobile-web-app-title content="CodeIT"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://shakex.github.io/thumbbox4/><link rel=prev href=https://shakex.github.io/lmsi-fivek/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"探索OCR——在印刷体表格识别的尝试","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/shakex.github.io\/thumbbox4\/"},"image":[{"@type":"ImageObject","url":"https:\/\/shakex.github.io\/thumbbox4\/featured-image.png","width":800,"height":600}],"genre":"posts","keywords":"计算机视觉, OCR, 算法","wordcount":7585,"url":"https:\/\/shakex.github.io\/thumbbox4\/","datePublished":"2021-04-17T00:00:00+00:00","dateModified":"2021-04-17T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"pudding"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=布丁布瓜><img class="lazyload logo" src=/svg/loading.min.svg data-src=/images/avatar.png data-srcset="/images/avatar.png, /images/avatar.png 1.5x, /images/avatar.png 2x" data-sizes=auto alt=/images/avatar.png title=/images/avatar.png>布丁布瓜</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/categories/>栏目 </a><a class=menu-item href=/project/>项目 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://tiddlywiki-teal.vercel.app rel="noopener noreferrer" target=_blank>Wiki </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=布丁布瓜><img class="lazyload logo" src=/svg/loading.min.svg data-src=/images/avatar.png data-srcset="/images/avatar.png, /images/avatar.png 1.5x, /images/avatar.png 2x" data-sizes=auto alt=/images/avatar.png title=/images/avatar.png>布丁布瓜</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i></a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i></a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/>文章</a><a class=menu-item href=/categories/>栏目</a><a class=menu-item href=/project/>项目</a><a class=menu-item href=/about/>关于</a><a class=menu-item href=https://tiddlywiki-teal.vercel.app rel="noopener noreferrer" target=_blank>Wiki</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">探索OCR——在印刷体表格识别的尝试</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://mp.weixin.qq.com/s/3ipKEIviWqt7Ul54tMz90g title=Author target=_blank rel="noopener noreferrer author" class=author><i class="fas fa-user-circle fa-fw"></i>pudding</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/%E6%8B%87%E6%8C%87%E7%9B%92/><i class="far fa-folder fa-fw"></i>拇指盒</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-04-17>2021-04-17</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 7585 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 16 分钟&nbsp;<span id=/thumbbox4/ class=leancloud_visitors data-flag-title=探索OCR——在印刷体表格识别的尝试>
<i class="far fa-eye fa-fw"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/thumbbox4/featured-image.png data-srcset="/thumbbox4/featured-image.png, /thumbbox4/featured-image.png 1.5x, /thumbbox4/featured-image.png 2x" data-sizes=auto alt=/thumbbox4/featured-image.png title=/thumbbox4/featured-image.png></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#摘要>摘要</a></li><li><a href=#简介>简介</a></li><li><a href=#方法要点>方法要点</a><ul><li><a href=#版面分析与文本检测>版面分析与文本检测</a></li><li><a href=#文本识别>文本识别</a><ul><li><a href=#数据集构建>数据集构建</a></li><li><a href=#crnn模型>CRNN模型</a></li></ul></li><li><a href=#文本纠错>文本纠错</a></li></ul></li><li><a href=#下阶段工作展望>下阶段工作展望</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></div><div class=content id=content><blockquote><p>去年年底在做的一个研究课题，有关于OCR财务报表识别。初次探索这个在市面上有不少落地产品的新鲜领域，简单总结一下课题的成果。[<a href=https://github.com/shakex/SheetOCR target=_blank rel="noopener noreferrer">项目地址</a>]</p></blockquote><h2 id=摘要>摘要</h2><p>OCR（Optical Character Recognition）光学字符识别，是指将图像上的文字转化为计算机可编辑的文字内容的技术，众多的研究人员对相关的技术研究已久，也有不少成熟的OCR技术和产品产生。本课题以上市公司近5年的财务报表为研究数据，主要探索了OCR在表格印刷体识别方向的一般方法和技术要点。在这一阶段（2020.10—2020.12）的研究和实践过程中，小组主要调研了市面上应用于印刷体识别的一般方法和算法模型，并初步搭建了一套用于财务报表智能识别和分析的系统。本报告针对其中的算法部分做简要介绍，主要涉及课题实现的整体思路、数据增强的技巧、文本检测和识别模型的结构、文本纠错的思路以及在实际实现中的难点和下一阶段的实施方向，作为阶段性的总结和分享。</p><h2 id=简介>简介</h2><p>图像文字中包含着丰富的语义信息，OCR（Optical Character Recognition）在金融行业的应用以文档相关的印刷体文本识别为主，如卡证、票据、合同、报表等。OCR作为流程自动化的重要一环，可以极大提升业务流程的效率，减少人工审核、录入的工作量，优化产品服务的客户体验，以及为后续的数据分析与挖掘提供从图像到文本的技术基础。目前市面上的OCR产品主要按照具体的应用场景提供服务接口。从产品和服务角度来看，不同行业、场景下的文本内容往往有其特定的排版标准，按照场景划分更加贴近业务需求，并能够针对性地解决业务痛点；从技术角度来看，早期的OCR主要基于传统图像处理方法，受技术限制其应用场景主要集中在车牌识别、文档扫描、银行卡卡号识别等单一领域。随着近年来以数据驱动的机器学习，尤其是深度学习在图像目标识别、自然语言处理上的研究取得突破进展，OCR在多场景的应用与落地成为可能。在学术界，针对全场景的通用文本检测与识别技术依然是有待研究与提升的方向，其主要面临的挑战有文字多样性（多语种、字体、颜色、大小、方向、形状）、复杂背景（背景图案与纹理、遮挡）、图像质量（拍摄角度、分辨率、噪声、光照）、数据条件（小样本、无监督）等；在工业界，针对具体细分场景的算法优化和与业务系统的融合为主要解决的问题。如下表所示，参考百度OCR产品的场景分类，我们可以看出，目前较为成熟的应用方向主要针对行业相关的印刷文档或制品。而随着学术研究在文本识别和检测的发展，OCR的应用场景和范围将进一步扩大，例如影视文字识别、店铺招牌识别、街景文字识别、通用手写体识别等。</p><table><thead><tr><th>产品</th><th>识别类</th></tr></thead><tbody><tr><td>卡证识别</td><td>身份证、银行卡、营业执照、户口本、护照等</td></tr><tr><td>汽车场景文字识别</td><td>行驶证、驾驶证、车牌、VIN码、车辆合格证、机动车销售发票等</td></tr><tr><td>财务票据文字识别</td><td>增值税发票、火车票、行程单、支票、汇票等</td></tr><tr><td>医疗票据文字识别</td><td>医疗发票、保险单、医疗费用结算单等</td></tr><tr><td>教育场景文字识别</td><td>手写文字、拍照搜题、作业及试卷文字与公示等</td></tr><tr><td>其他场景</td><td>仪器仪表盘读数、印章、彩票等</td></tr></tbody></table><p>目前OCR算法的核心包括文本检测和文本识别两个步骤。文本检测定位图像中的文字位置，一般用矩形框或四边形标记文本所在区域；文本识别根据检测得到的文本图像，由识别模型得到对应的文本信息。为了解决通用文本检测与识别中的挑战，近年来研究人员在各个研究方向都取得了较大的进展。数据方面，无论是检测还是识别模型，识别精度和效果很大程度上依赖于数据规模和数据的多样性——即在使用相同模型的情况下，训练数据越多，数据分布越广，训练的模型就具有更好的识别和泛化性能。因此，大量数据的获取、分类、清洗和标注工作对于最终取得良好的识别效果至关重要。在实际生产中，用于训练一个模型所需的图像数据往往是百万级别的，且需要耗费较多的人力和时间去完成数据标注。在合适的情况下，利用现有的公开数据集是一种可行的解决方案；此外，在很多领域，出于隐私保护、成本控制等原因，并不能获取大量的场景数据，利用数据增广方法可以解决此类样本缺失或不均衡的问题。模型方面，本文检测模型主要分为基于检测框和基于Mask两类，典型算法包括：<a href=https://link.springer.com/chapter/10.1007/978-3-319-46484-8_4 target=_blank rel="noopener noreferrer">CTPN (Tian et al., 2016)</a> 、<a href=https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf target=_blank rel="noopener noreferrer">EAST (Zhou et al., 2017)</a>、PMTD、DB等；文本识别模型分为基于CTC（Connectionist Temporal Classification）的模型（<a href=https://arxiv.org/pdf/1507.05717.pdf target=_blank rel="noopener noreferrer">Shi et al., 2017b（CRNN）</a> 、<a href=https://arxiv.org/pdf/1506.04395.pdf target=_blank rel="noopener noreferrer">He et al., 2016（DTRN）</a>、<a href=https://arxiv.org/abs/1709.04303 target=_blank rel="noopener noreferrer">Gao et al., 2017</a>、<a href=https://arxiv.org/abs/1709.01727 target=_blank rel="noopener noreferrer">Yin er al., 2017</a>等）和基于注意力机制的编码器-解码器结构（<a href=https://arxiv.org/pdf/1603.03101.pdf target=_blank rel="noopener noreferrer">Lee and Osindero, 2016</a>、<a href=https://arxiv.org/abs/1711.04226 target=_blank rel="noopener noreferrer">Cheng et al., 2018</a>、<a href=https://arxiv.org/pdf/1805.03384.pdf target=_blank rel="noopener noreferrer">Bai et al., 2018</a>、<a href=https://ren-fengbo.lab.asu.edu/sites/default/files/16354-77074-1-pb.pdf target=_blank rel="noopener noreferrer">Liu et al., 2018d</a>）。此外，目前也有不少针对不规则文本检测和识别的方法（<a href=http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Geometry-Aware_Scene_Text_CVPR_2018_paper.pdf target=_blank rel="noopener noreferrer">Wang et al., 2018（ITN）</a>、<a href=https://arxiv.org/abs/1904.06535 target=_blank rel="noopener noreferrer">Zhang et al., 2019</a>、<a href=https://arxiv.org/abs/1905.05980 target=_blank rel="noopener noreferrer">Wang et al., 2019b</a>）和将检测和识别模型联合训练合并成为端到端网络的工作（Bartz et al., 2017、Xing et al., 2019）。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg data-sub-html="<h2>图1 表格印刷体识别的一般流程</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1goyui3g5wcj30860nkta9.jpg width=150></a><figcaption class=image-caption>图1 表格印刷体识别的一般流程</figcaption></figure><p>课题小组从目前较为成熟的文档印刷体识别方向出发，总结文档类OCR算法构建的主要步骤如图1所示：输入一张通过图像采集设备获取的文档图像，一般需要经历以下五个步骤：图像校正、版面分析、文本检测、文本识别和文本纠错，最终得到所需要提取的文字信息。具体如下：</p><ul><li><p><strong>图像校正</strong>：主要利用灰度化、二值化、图像平滑、直线检测、边缘检测、透视变化等一些方法对采集的文档可能存在的倾斜、遮挡等情况进行校正。该步骤的一个主要难点在于准确定位图像主体（文档/卡证）的四个角点，并利用角点坐标对主体进行校正。针对复杂背景和角点部分遮挡的情况，需要训练分割或边缘检测模型以获得较好的校正效果。例如，图2展示了对于身份证件照片进行透视变化校正，得到用于文本检测和识别的图像。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg data-sub-html="<h2>图2 身份证图像校正</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1gp033or762j31260gi4qp.jpg></a><figcaption class=image-caption>图2 身份证图像校正</figcaption></figure></li><li><p><strong>版面分析</strong>：版面分析可以对文档内的图像、文本、公式、表格信息和位置关系进行自动分析、识别和理解。对于复杂的文档，根据版面分析的不同区块采用不同的检测与分割模型，可以提高OCR系统的识别准确率。例如，图3展示了对于一篇论文版面结构的划分。</p><figure><a class=lightgallery href=https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg title=https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg data-thumbnail=https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg data-sub-html="<h2>图 3 论文图像的版面分析</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg data-srcset="https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg, https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg 1.5x, https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg 2x" data-sizes=auto alt=https://pic4.zhimg.com/v2-3d0977561c8dcf6b7282f9531576925f_b.jpg></a><figcaption class=image-caption>图 3 论文图像的版面分析</figcaption></figure></li><li><p><strong>文本检测</strong>：文本检测识别并定位文本在图像中的位置。对于规则的印刷体文本，通常以矩形框标注出每个文本目标的边框。方法主要分为基于检测框和基于Mask两类，基于检测框的文本检测，其思路是先利用若干 Anchor 产生大量候选文本框，再经过 NMS 得到最终结果；基于 Mask 的文本检测，其思路是通过分割网络进行像素级别的语义分割，再通过后处理得到文本框，由于后处理比较复杂，这个步骤会直接影响基于 Mask 文本检测算法的性能。</p></li><li><p><strong>文本识别</strong>：文本识别将文本检测得到的矩形框作为文本识别模型的输入，通过识别模型得到的该矩形区域内的文本内容。由于输入图像中所包含的文字数量是不确定的，因此文本识别的一个关键是解决输入图像宽度与文本字符长度的对应关系，识别模型分为两类：一类是基于CTC（Connectionist Temporal Classification）的模型，有（<a href=https://arxiv.org/pdf/1507.05717.pdf target=_blank rel="noopener noreferrer">Shi et al., 2017b（CRNN）</a> 、<a href=https://arxiv.org/pdf/1506.04395.pdf target=_blank rel="noopener noreferrer">He et al., 2016（DTRN）</a>、<a href=https://arxiv.org/abs/1709.04303 target=_blank rel="noopener noreferrer">Gao et al., 2017</a>、<a href=https://arxiv.org/abs/1709.01727 target=_blank rel="noopener noreferrer">Yin er al., 2017</a>等），另一类是基于意力机制的编码器-解码器结构（<a href=https://arxiv.org/pdf/1603.03101.pdf target=_blank rel="noopener noreferrer">Lee and Osindero, 2016</a>、<a href=https://arxiv.org/abs/1711.04226 target=_blank rel="noopener noreferrer">Cheng et al., 2018</a>、<a href=https://arxiv.org/pdf/1805.03384.pdf target=_blank rel="noopener noreferrer">Bai et al., 2018</a>、<a href=https://ren-fengbo.lab.asu.edu/sites/default/files/16354-77074-1-pb.pdf target=_blank rel="noopener noreferrer">Liu et al., 2018d</a>）。图4 展示了利用检测和识别算法对于中文表格和英文文本的识别结果。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg title=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg data-sub-html="<h2>图4 文本检测与识别示例1</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg, https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0q64wnnj31ic0qyb29.jpg></a><figcaption class=image-caption>图4 文本检测与识别示例1</figcaption></figure><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg title=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg data-sub-html="<h2>图4 文本检测与识别示例2</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg, https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0shrey2j31bg0u0npd.jpg></a><figcaption class=image-caption>图4 文本检测与识别示例2</figcaption></figure></li><li><p><strong>文本纠错</strong>：对于一些复杂场景和特殊字符，识别模型往往很难达到100%的准确率，因此利用语言作为先验信息，对识别结果进行纠错，可以增加OCR系统的鲁棒性。OCR识别纠错主要涉及文本的词法错误，例如，识别模型将“上海市”识别为“上海币”。纠错的两个关键问题分别为：语言模型和字形的相似度度量。语言模型给出当前识别结果最可能的几个真实值，字形的相似度度量给出真实值识别为当前结果的可能性。</p></li></ul><p>当然，上述步骤并不是对于所有场景都是不可或缺，针对具体文档图像依然需要从实际问题出发，选择并设计适合的处理方法和流程。</p><h2 id=方法要点>方法要点</h2><p>课题组实现的OCR任务如下：给定<strong>450</strong>张经过预处理的多家上市公司近五年财报，其中分为利润表、资产负债表和现金流量表三种类型各<strong>150</strong>张，每张图像包含标题、表头信息（证券代码、证券简称、报告日期、单位）和表身信息，要求识别每张图像中的文本内容，将识别结果写入CSV文件中。交互设计页面呈现如图5所示：其中左侧为输入图像的部分截图，右侧为界面显示的识别结果。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg data-sub-html="<h2>图5 财务报表OCR识别及其界面呈现</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1goyusnsfbtj317s0hw47m.jpg></a><figcaption class=image-caption>图5 财务报表OCR识别及其界面呈现</figcaption></figure><p>由于给定的处理图像质量较好，不存在倾斜、透视等情况，因此省去了上一节所提到的图像校正步骤。针对此场景，我们进行处理的主要步骤如下：</p><ol><li><p>通过基于阈值的图像处理方法对表格中的文本进行检测，并同时确定对应文本所属区域（表头或表身）与键-值对应关系（版面分析+文本检测）</p></li><li><p>利用CRNN模型对检测文本进行识别（文本识别）</p></li><li><p>对识别结果进行纠错，主要涉及金额项中逗号和点的纠错（文本纠错）</p></li></ol><h3 id=版面分析与文本检测>版面分析与文本检测</h3><p>给定图像只包含单张表格，且背景统一为白色。针对此类情况，我们可以直接利用基于阈值的方法划分表头和表身部分，同时裁剪出表身每个单元格内文本。具体步骤如下：</p><ol><li><p>灰度化</p></li><li><p>形态学算子去除噪点</p></li><li><p>图像行扫描，确定第一条直线位置，其上为表头信息部分，其下为表格主体部分</p></li><li><p>表头信息文本检测：行列扫描，根据灰度阈值切割文本</p></li><li><p>表身信息文本检测：图像行列扫描，确定每一单元格的位置；在每个单元格内，切割出对应文本矩形框</p></li><li><p>对所有矩形框进行微调：调整宽高大小，确保文本有效信息不丢失。统一矩形框的高度</p></li><li><p>保存检测框的位置信息，匹配文本的键-值对，记录信息至XML文件</p></li></ol><p>检测结果可视化如图6所示。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg data-sub-html="<h2>图6 财务报表文本检测结果</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1gp3lw1m1tbj30n80pa76r.jpg></a><figcaption class=image-caption>图6 财务报表文本检测结果</figcaption></figure><h3 id=文本识别>文本识别</h3><h4 id=数据集构建>数据集构建</h4><p>为了训练适应于当前应用域的文本识别模型，我们首先构建模型训练所需的数据集。构建数据集包括两部分工作：1. 字符集构建；2. 文本短语图像集及标注。其中，字符集的构建，需要收集样本中所用到的所有字符。针对课题所提供的报表图像，我们共统计出中文字符342个，英文+数字字符69个，共计411个字符（当然，为了训练能够认识更多中文字符的模型，需要扩展字符集。通常来说，最常用的中文字符在3000字左右）。对于文本短语数据集的构建，在对450张表格图像进行文本检测以后，得到约21w张短语图像。其中，由于不同财报图像中包含大量重复短语，因此需要对检测进行去重。去重以后图像样本在1.7w张左右，但是，样本中绝大部分为数字样本，中文短语样本仅378张。显然，如果仅使用这些数据构建训练集的话，会存在样本不均衡的问题，即数据分布集中在数字字符类别，而对于大量的中文字符类，数据样本过少，这显然与真实情况的样本分布相差甚远，会造成训练的模型对于数字样本的过拟合，图7显示了在仅利用现有数据进行模型训练（采用CRNN模型）以后的预测结果，可以看到，结果中对于数字的识别较为准确（部分字符仍然出现识别错误，说明对于数字样本而言，数据样本也需要一定程度的增广），但是对于中文的识别准确率很不理想；因此，我们需要对数据进行增广，扩大中文样本的数量，使得每个字符类的分布尽量均衡。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg title=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg data-sub-html="<h2>图 7 数据样本不均衡情况下的模型训练预测结果</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg, https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEly1gpn0glpxupj315s08oq5y.jpg></a><figcaption class=image-caption>图 7 数据样本不均衡情况下的模型训练预测结果</figcaption></figure><p>在进行数据增广之前，我们对现有数据的基本情况及分布做了基本统计。图8展示了短语样本长度的分布。对于数字样本，其长度分布主要集中在约15-20个字符之间；对于中文短语样本，其长度分布主要集中在约4-10个字符之间。图9展示了在所有样本中出现频次较高的字符和单词统计。对于0-9的数字字符，字符1-9出现的频次大体相同，字符0出现的频次是字符1-9出现频次的3倍，这大致符合数字作为金额时的分布规律。在关于中文字符的统计中，可以看到，出现频次前三字符为“金”，“资”，“现”，出现频次前三的单词为“现金”，“其他”，“资产”。由此可见，针对财务报表的场景，在所构建的数据集中适当提高财会类字符或词汇的占比，可以更加贴近数据的真实分布。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg data-sub-html="<h2>图 8 数据样本长度分布统计</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1goz003edklj30uq0audhi.jpg></a><figcaption class=image-caption>图 8 数据样本长度分布统计</figcaption></figure><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg data-sub-html="<h2>图 9 数据样本高频字符和单词统计</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1goz00uw5rgj319s0is42k.jpg></a><figcaption class=image-caption>图 9 数据样本高频字符和单词统计</figcaption></figure><p>接下来考虑文本数据的增广方法，数据增广的操作可以分为两大类：一类是对现有数据进行图像增强，如旋转、拉伸、对比度变化、模糊、膨胀、腐蚀等操作，这类操作可以在维持数据现有分布的情况下，提高模型的泛化性和鲁棒性；另一类是通过文本生成算法去产生新的文本图像，这类操作调整数据更加趋近真实场景的数据分布。在课题组的研究过程中，我们尝试了论文“EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks”所提出的数据生成方法，并在此基础上做了适当修改。该方法针对给定短语文本，可以通过四种基本操作（随机插入、随机删除、随机替换、随机交换）产生新的短语文本，具体操作如下：</p><ul><li><p><strong>随机插入（Random Insertion, RI）</strong>:在每个短语的单词间隙中随机插入n个新单词（短语长度增加）</p></li><li><p><strong>随机删除（Random Deletion, RD）</strong>：以概率p随机删除短语中所包含的单词（短语长度减少）</p></li><li><p><strong>随机替换（Random Replacement, RR）</strong>：用一个新单词替代短语中的任意一个单词，重复n次（短语长度几乎不变）</p></li><li><p><strong>随机交换（Random Swap, RS）</strong>：随机选取短语中的两个单词进行交换，重复n次（短语长度不变）</p></li></ul><p>例如，对于“一年内到期的非流动资产”这个文本短语，利用EDA随机得到的文本如下：</p><table><thead><tr><th></th><th>RI</th><th>RD</th><th>RR</th><th>RS</th></tr></thead><tbody><tr><td>示例1</td><td>一年内到期税费的非流动资产</td><td>非流动</td><td>一年内川能动力的非流动资产</td><td>一年内资产的非流动到期</td></tr><tr><td>示例2</td><td>代码一年内到期的汇率非流动资产</td><td>一年内的</td><td>一年内其他的非流动资产</td><td>到期一年内的非流动资产</td></tr></tbody></table><p>最后，我们对所有的数据样本进行文本标注，并划分训练和验证集合。由于手工标注工作量比较大，我们选择直接利用市面上的API做文本识别，得到标注结果，再针对一些标注错误的情况进行修改。对于数据集的划分，我们按照7:3的比例随机划分训练和验证集合。</p><h4 id=crnn模型>CRNN模型</h4><p>在完成数据集的构建后，我们着手探究适用于印刷体文本识别的模型构建。在这一阶段，课题组尝试了运用最广泛的CRNN模型，图11展示了该模型的结构。CRNN模型主要分为三部分：1. <strong>卷积网络层</strong>。主要使用CNN对输入图像进行特征提取，得到特征图；2. <strong>循环网络层</strong>。主要利用RNN（BiLSTM）对特征序列进行预测，对序列中的每个特征向量进行学习，并输出预测标签（真实值）分布；3. <strong>转录层</strong>。主要利用CTC解决针对不同输入长度的文本字符对其问题，使用CTC损失把循环网络层获取的一系列标签分布转换成最终的标签序列。模型技术细节可参考<a href=https://arxiv.org/pdf/1507.05717.pdf target=_blank rel="noopener noreferrer">CRNN论文</a>。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg data-sub-html="<h2>图 10 CRNN模型结构</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1goz0lyzrv4j30g50jh76q.jpg width=400></a><figcaption class=image-caption>图 10 CRNN模型结构</figcaption></figure><h3 id=文本纠错>文本纠错</h3><p>通过识别模型的得到的预测结果往往不能达到100%的识别准确率，在文本纠错步骤中可以结合一些先验知识对识别结果进行改进：</p><ul><li><p>针对表格中的金额，“,”和“.”字符比较接近，识别结果容易预测出错，可以金额中逗号出现的规则设计算法进行纠正；</p></li><li><p>预测文本中的括号缺失问题，可通过括号匹配算法进行纠正；</p></li><li><p>对预测文本中的日期、金额、公司名、证券代码等关键字段设计算法进行校验，对于不合法文本，采用多模型预测融合的策略进行纠正；</p></li><li><p>其他词法错误，通过训练语言模型进行纠正。</p></li></ul><p>基于以上步骤，课题小组在现阶段实现对于财务报表的识别结果如图11所示。</p><figure><a class=lightgallery href=https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg title=https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg data-thumbnail=https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg data-sub-html="<h2>图 11 利润表的OCR识别结果</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg data-srcset="https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg, https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg 1.5x, https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg 2x" data-sizes=auto alt=https://tva1.sinaimg.cn/large/008eGmZEgy1gpmkzp6wrhj30u00unanp.jpg></a><figcaption class=image-caption>图 11 利润表的OCR识别结果</figcaption></figure><h2 id=下阶段工作展望>下阶段工作展望</h2><p>针对课题组在OCR算法方面的工作，下阶段主要的改进有：</p><ul><li><p>数据方面，经过EDA数据增广后中文数据的样本量有了很大提升，但距离实际生产所需的数据量（百万级）还差很远。课题组还将尝试采用引入外部数据集与多种数据增广方法相结合的方式，建立适用于金融领域的文本识别数据库。</p></li><li><p>模型方面，现阶段课题组验证了CRNN模型在文本识别上的可行性，后续将测试并改进更多文本识别模型，构建模型库，并基于文本数据集完成模型训练和调优，提供可供系统调用的统一接口。</p></li><li><p>部署方面，现阶段的部署方案采用客户端与服务端共享文件夹，OCR程序后台轮询文件夹的方式实现。在下一阶段，将实现通过http请求方式调用服务。</p></li></ul><h2 id=参考文献>参考文献</h2><p><strong>综合</strong></p><ul><li>Long, Shangbang, Xin He and Cong Yao. “Scene Text Detection and Recognition: The Deep Learning Era.” International Journal of Computer Vision 129 (2020): 161-184.</li><li>Du, Yuning, Chenxia Li, Ruoyu Guo, X. Yin, Weiwei Liu, Jun Zhou, Y. Bai, Z. Yu, Y. Yang, Qingqing Dang and Haoshuang Wang. “PP-OCR: A Practical Ultra Lightweight OCR System.” ArXiv abs/2009.09941 (2020): n. pag.</li></ul><p><strong>表格识别</strong></p><ul><li>Li, Y., Z. Huang, Junchi Yan, Y. Zhou, Fan Ye and Xianhui Liu. “GFTE: Graph-based Financial Table Extraction.” ICPR Workshops (2020).</li><li>Paliwal, Shubham, D. Vishwanath, R. Rahul, M. Sharma and L. Vig. “TableNet: Deep Learning Model for End-to-end Table Detection and Tabular Data Extraction from Scanned Document Images.” 2019 International Conference on Document Analysis and Recognition (ICDAR) (2019): 128-133.</li><li>Khan, Saqib Ali, Syed Khalid, M. Shahzad and F. Shafait. “Table Structure Extraction with Bi-Directional Gated Recurrent Unit Networks.” 2019 International Conference on Document Analysis and Recognition (ICDAR) (2019): 1366-1371.</li></ul><p><strong>图像校正（边缘检测）</strong></p><ul><li>Xie, Saining and Zhuowen Tu. “Holistically-Nested Edge Detection.” International Journal of Computer Vision 125 (2015): 3-18.</li></ul><p><strong>版面分析</strong></p><ul><li>Viana, Matheus Palhares and D. Oliveira. “Fast CNN-Based Document Layout Analysis.” 2017 IEEE International Conference on Computer Vision Workshops (ICCVW) (2017): 1173-1180.</li><li>Xu, Yiheng, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei and M. Zhou. “LayoutLM: Pre-training of Text and Layout for Document Image Understanding.” Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining(2020): n. pag.</li><li>Zhong, Xu, J. Tang and Antonio Jimeno-Yepes. “PubLayNet: Largest Dataset Ever for Document Layout Analysis.” 2019 International Conference on Document Analysis and Recognition (ICDAR) (2019): 1015-1022.</li></ul><p><strong>文本检测</strong></p><ul><li>Zhou, X., C. Yao, H. Wen, Yuzhi Wang, Shuchang Zhou, Weiran He and Jiajun Liang. “EAST: An Efficient and Accurate Scene Text Detector.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 2642-2651.</li><li>Liao, Minghui, Zhaoyi Wan, C. Yao, Kai Chen and X. Bai. “Real-time Scene Text Detection with Differentiable Binarization.” AAAI (2020).</li><li>Tian, Zhi, Weilin Huang, Tong He, Pan He and Yu Qiao. “Detecting Text in Natural Image with Connectionist Text Proposal Network.” ECCV (2016).</li></ul><p><strong>文本识别</strong></p><ul><li>Shi, Baoguang, X. Bai and Cong Yao. “An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition.” IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (2017): 2298-2304.</li><li>Lee, Chen-Yu and Simon Osindero. “Recursive Recurrent Nets with Attention Modeling for OCR in the Wild.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 2231-2239.</li></ul><p><strong>文本检测+识别端到端</strong></p><ul><li>Bartz, C., Haojin Yang and C. Meinel. “SEE: Towards Semi-Supervised End-to-End Scene Text Recognition.” AAAI (2018).</li><li>Busta, M., Lukás Neumann and Jiri Matas. “Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework.” 2017 IEEE International Conference on Computer Vision (ICCV) (2017): 2223-2231.</li><li>Xing, Linjie, Zeyong Tian, Weilin Huang and M. Scott. “Convolutional Character Networks.” 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019): 9125-9135.</li></ul><p><strong>文本纠错</strong></p><ul><li>Yu, Junjie and Zhenghua Li. “Chinese Spelling Error Detection and Correction Based on Language Model, Pronunciation, and Shape.” CIPS-SIGHAN (2014).</li></ul><p><strong>数据处理</strong></p><ul><li>Wei, Jason and K. Zou. “EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.” ArXiv abs/1901.11196 (2019): n. pag.</li></ul><p><strong>开源软件</strong></p><ul><li>国内OCR服务商: <a href="https://ai.baidu.com/tech/ocr?track=cp:ainsem%7cpf:pc%7cpp:chanpin-wenzishibie%7cpu:wenzishibie-baiduocr%7cci:%7ckw:10002846" target=_blank rel="noopener noreferrer">百度</a>/<a href="https://cloud.tencent.com/act/event/ocrsale?fromSource=gwzcw.4251956.4251956.4251956&utm_medium=cpc&utm_id=gwzcw.4251956.4251956.4251956" target=_blank rel="noopener noreferrer">腾讯</a>/<a href="https://ai.aliyun.com/ocr?spm=5176.182739.artificialIntelligence.20.69111d8aNKn80V" target=_blank rel="noopener noreferrer">阿里巴巴</a>/<a href=https://www.4paradigm.com/operate/smart-archive target=_blank rel="noopener noreferrer">第四范式</a>/<a href=https://support.huaweicloud.com/ocr/index.html target=_blank rel="noopener noreferrer">华为</a>/<a href=https://www.faceplusplus.com.cn/general-text-recognition/ target=_blank rel="noopener noreferrer">旷视</a>/<a href="https://www.sensetime.com/cn/product-detail?categoryId=32132" target=_blank rel="noopener noreferrer">商汤</a>/<a href="https://www.xfyun.cn/service/textRecg?ch=bd17-08&bd_vid=10414311197403882885" target=_blank rel="noopener noreferrer">科大讯飞</a>/<a href=https://www.intsig.com/public/ai_new/card.shtml target=_blank rel="noopener noreferrer">合合信息</a>/<a href=http://ai.exocr.com target=_blank rel="noopener noreferrer">易道博识</a>/<a href="http://www.hw-ai.com/index.php?m=content&c=index&a=lists&catid=23" target=_blank rel="noopener noreferrer">汉王数字</a>/<a href=https://ai.youdao.com/product-ocr-print.s target=_blank rel="noopener noreferrer">有道智云</a></li><li>开源OCR：<a href=https://github.com/JaidedAI target=_blank rel="noopener noreferrer">JaidedAI</a>/<strong><a href=https://github.com/JaidedAI/EasyOCR target=_blank rel="noopener noreferrer">EasyOCR</a></strong>, <a href=https://github.com/PaddlePaddle target=_blank rel="noopener noreferrer">PaddlePaddle</a>/<strong><a href=https://github.com/PaddlePaddle/PaddleOCR target=_blank rel="noopener noreferrer">PaddleOCR</a></strong>, <a href=https://github.com/tesseract-ocr target=_blank rel="noopener noreferrer">tesseract-ocr</a>/<strong><a href=https://github.com/tesseract-ocr/tesseract target=_blank rel="noopener noreferrer">tesseract</a></strong>, <a href=https://github.com/chineseocr target=_blank rel="noopener noreferrer">chineseocr</a>/<strong><a href=https://github.com/chineseocr/chineseocr target=_blank rel="noopener noreferrer">chineseocr</a></strong>, <a href=https://github.com/DayBreak-u target=_blank rel="noopener noreferrer">DayBreak-u</a>/<strong><a href=https://github.com/DayBreak-u/chineseocr_lite target=_blank rel="noopener noreferrer">chineseocr_lite</a></strong>, <a href=https://ocr.space/OCRAPI target=_blank rel="noopener noreferrer">OCR.space</a>, <a href=https://github.com/AvensLab target=_blank rel="noopener noreferrer">AvensLab</a>/<strong><a href=https://github.com/AvensLab/OcrKing target=_blank rel="noopener noreferrer">OcrKing</a></strong>, <a href=https://github.com/open-mmlab target=_blank rel="noopener noreferrer">open-mmlab</a>/<strong><a href=https://github.com/open-mmlab/mmocr target=_blank rel="noopener noreferrer">mmocr</a></strong></li><li>数据增广：<a href=https://github.com/oh-my-ocr target=_blank rel="noopener noreferrer">oh-my-ocr</a>/<strong><a href=https://github.com/oh-my-ocr/text_renderer target=_blank rel="noopener noreferrer">text_renderer</a></strong>, <a href=https://github.com/jasonwei20 target=_blank rel="noopener noreferrer">jasonwei20</a>/<strong><a href=https://github.com/jasonwei20/eda_nlp target=_blank rel="noopener noreferrer">eda_nlp</a></strong></li><li>文本纠错：<a href=https://github.com/shibing624 target=_blank rel="noopener noreferrer">shibing624</a>/<strong><a href=https://github.com/shibing624/pycorrector target=_blank rel="noopener noreferrer">pycorrector</a></strong>, <a href=https://github.com/iqiyi target=_blank rel="noopener noreferrer">iqiyi</a>/<strong><a href=https://github.com/iqiyi/FASPell target=_blank rel="noopener noreferrer">FASPell</a></strong></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2021-04-17</span></div><div class=post-info-license><span>CC BY-NC 4.0</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://shakex.github.io/thumbbox4/ data-title=探索OCR——在印刷体表格识别的尝试 data-hashtags=计算机视觉,OCR,算法><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://shakex.github.io/thumbbox4/ data-hashtag=计算机视觉><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://shakex.github.io/thumbbox4/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://shakex.github.io/thumbbox4/ data-title=探索OCR——在印刷体表格识别的尝试 data-ralateuid=shake1110><i class="fab fa-weibo fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/>计算机视觉</a>,&nbsp;<a href=/tags/ocr/>OCR</a>,&nbsp;<a href=/tags/%E7%AE%97%E6%B3%95/>算法</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/lmsi-fivek/ class=prev rel=prev title="MIT-Adobe FiveK dataset"><i class="fas fa-angle-left fa-fw"></i>MIT-Adobe FiveK dataset</a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://github.com/sunt-programator/CodeIT target=_blank>Hugo-CodeIT</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank rel="noopener noreferrer">shake</a></span>
<span id=timeDate>| <a href=https://mp.weixin.qq.com/s/4Vr5vI8-5xKjcPpwZIZzAQ target=_blank rel="noopener noreferrer">第 time 天</a>&nbsp;&nbsp;</span>
<script>var now=new Date();function createtime(){var start_time=new Date("01/23/2021 00:00:00");now.setTime(now.getTime()+250);days=(now-start_time)/1000/60/60/24;dnum=Math.floor(days);var worktime=document.getElementById("timeDate").innerHTML.replace(/time/,Math.floor(days));document.getElementById("timeDate").innerHTML=worktime;}
createtime();</script></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i></a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/lib/valine/Valine.min.js></script><script type=text/javascript src=/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lunr/lunr.stemmer.support.min.js></script><script type=text/javascript src=/lib/lunr/lunr.zh.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/lightgallery/lightgallery.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-thumbnail.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-zoom.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/auto-render.min.js></script><script type=text/javascript src=/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/mhchem.min.js></script><script type=text/javascript src=/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":20},"comment":{"valine":{"appId":"DpDp52Y2M2hvbaNVVVlm36UW-gzGzoHsz","appKey":"DsVUBKul7XnmLg4QY8qeywWh","avatar":"retro","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-apple@5.0.1/img/apple/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"zh-cn","pageSize":10,"placeholder":"说点什么...","recordIP":true,"visitor":true}},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type=text/javascript src=/js/theme.min.js></script></body></html>