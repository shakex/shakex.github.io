<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shake and Shake&#39;s Vision</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xiekai.site/"/>
  <updated>2018-02-06T13:40:14.357Z</updated>
  <id>http://xiekai.site/</id>
  
  <author>
    <name>Shake</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>test4</title>
    <link href="http://xiekai.site/2018/02/06/test3-1/"/>
    <id>http://xiekai.site/2018/02/06/test3-1/</id>
    <published>2018-02-06T06:22:26.000Z</published>
    <updated>2018-02-06T13:40:14.357Z</updated>
    
    <content type="html"><![CDATA[<h1 id="This-is-a-h1-title"><a href="#This-is-a-h1-title" class="headerlink" title="This is a h1 title"></a>This is a h1 title</h1><h2 id="This-is-a-h2-title"><a href="#This-is-a-h2-title" class="headerlink" title="This is a h2 title"></a>This is a h2 title</h2><h3 id="This-is-a-h3-title"><a href="#This-is-a-h3-title" class="headerlink" title="This is a h3 title"></a>This is a h3 title</h3><h4 id="This-is-a-h4-title"><a href="#This-is-a-h4-title" class="headerlink" title="This is a h4 title"></a>This is a h4 title</h4><h5 id="This-is-a-h5-title"><a href="#This-is-a-h5-title" class="headerlink" title="This is a h5 title"></a>This is a h5 title</h5><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><p><strong>Paragraph</strong> In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, as we will see later in the course, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification.</p><ul><li>A single instance of an object can be oriented in many ways with respect to the camera.</li><li>Many objects of interest are not rigid bodies and can be deformed in extreme ways.</li><li>The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.</li><li>The effects of illumination are drastic on the pixel level.<ul><li>Background clutter</li><li>Intra-class variation</li></ul><ol><li>Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set.</li><li>Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model.</li></ol></li></ul><h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><p>You are probably familiar with the concept of a derivative in the scalar case: given a function $f : \mathbb{R} \rightarrow \mathbb{R}$, the derivative of f at a point $x ∈ \mathbb{R}$ is defined as:</p><script type="math/tex; mode=display">y(x,w)=w_0+\sum_{j=1}^{M-1}w_j \phi_j(x) \tag{1-1}</script><p>Derivatives are a way to measure change. In the scalar case, the derivative of the function $f$ at the point $x$ tells us how much the function $f$ changes as the input $x$ changes by a small amount $\varepsilon$:</p><script type="math/tex; mode=display">y(x,w)=\sum_{j=0}^{M-1}w_j \phi_j(x)=w^T\phi(x) \tag{1-2}</script><p>For ease of notation we will commonly assign a name to the output of $f$, say $y = f(x)$, and write $\frac{\partial{y}}{\partial{x}}$ for the derivative of $y$ with respect to $x$. This $\partial{x}$ notation emphasizes that $\frac{\partial{y}}{\partial{x}}$ is the <em>rate of change</em> between the variables $x$ and $y$; Concretely if $x$ were to change by $\varepsilon$ then $y$ will change by approximately $\varepsilon \frac{\partial{y}}{\partial{x}}$. We can write this relationship as</p><script type="math/tex; mode=display">\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}} \tag{1-3}</script><h2 id="中文与公式"><a href="#中文与公式" class="headerlink" title="中文与公式"></a>中文与公式</h2><p>过拟合是指学习时选择的模型所包含的参数过多，导致对已知数据预测得很好，但对未知数据预测很差的现象。即所得的假设函数拥有较低的训练误差，但有较高的泛化误差。导致过拟合的主要影响因素有：模型的复杂度、训练数据的噪声、数据规模。过拟合无法彻底避免，只能减小其风险。</p><p>最大似然估计就是用似然函数取到最大值时的参数值作为估计值，似然函数可以写做：</p><script type="math/tex; mode=display">L(\theta)=p(X | \theta)=\prod_{i=n}^N p(x_n | \theta) \tag{2-1}</script><p>为防止浮点数下溢与方便数值求解，通常取对数似然：</p><script type="math/tex; mode=display">LL(\theta)=\sum_{n=1}^N \ln p(x_n | \theta) \tag{2-2}</script><p>所以，$\hat{\theta}_{ML}$ 表示为（取平均对数似然）：</p><script type="math/tex; mode=display">\hat{\theta}_{ML}=\arg\max_\theta \frac{1}{N} \sum_{n=1}^N \ln p(x_n | \theta) \tag{2-3}</script><p>当使用数值方法求解极小值时，通常会将最大化问题转化为最小化问题，即取负对数。</p><h2 id="中文文章"><a href="#中文文章" class="headerlink" title="中文文章"></a>中文文章</h2><p>&#160; &#160; &#160; &#160;我舅舅上个世纪（20世纪）末生活在世界上。有件事我们大家都知道：在中国，历史以三十年为极限，我们不可能知道三十年以前的事。我舅舅比我大了三十多岁，所以他的事我就不大知道——更正确的说法是不该知道。他留下了一大堆的笔记、相片，除此之外，我还记得他的样子。他是个肤色黝黑的大个子，年轻时头发很多，老了就秃了。他们那个时候的事情，我们知道的只是：当时烧煤，烧得整个天空乌烟障气，而且大多数人骑车上班。自行车这种体育器械，在当年是一种代步工具，样子和今天的也大不相同，在两个轮子之间有一个三角形的钢管架子，还有一根管子竖在此架子之上。流传到现在的车里有一小部分该管子上面有个车座，另一部分上面什么都没有；此种情形使考古学家大惑不解，有人说后一些车子的座子遗失了，还有人提出了更深刻的解释——当时的人里有一部分是受信任的，可以享受比较好的生活，有座的车就属于他们。另一部分人不受信任，所以必须一刻不停地折磨自己，才能得到活下去的权利，故而这种不带座子的自行车就是他们对肛门、会阴部实施自残自虐的工具。根据我的童年印象，这后一种说法颇为牵强。我还记得人们是怎样骑自行车的。但是我不想和权威争辩——上级现在还信任我，我也不想自讨没趣。</p><p>&#160; &#160; &#160; &#160;舅舅是个作家，但是在他生前一部作品也没发表过，这是他不受信任的铁证。因为这个原故，他的作品现在得以出版，并且堆积在书店里无人问津。众所周知，现在和那时大不一样了，我们的社会发生了重大转折，走向了光明。——不管怎么说吧，作为外甥，我该为此大为欢喜，但是书商恐怕会有另一种结论。我舅舅才情如何，自然该由古典文学的研究者来评判，我知道的只是：现在纸张书籍根本不受欢迎，受欢迎的是电子书籍，还该有多媒体插图。所以书商真的要让我舅舅重见天日的话，就该多投点资，把我舅舅的书编得像点样子。现在他们又找到我，让我给他老人家写一本传记，其中必须包括他骑那种没有座的自行车，并且要考据出他得了痔疮，甚至前列腺癌。但是根据我掌握的材料，我舅舅患有各种疾病，包括关节炎、心脏病，但上述器官没有一种长在肛门附近，是那种残酷的车辆导致的。他死于一次电梯事故，一下子就被压扁了，这是个让人羡慕的死法，明显地好于死于前列腺癌。这就使我很为难了。我本人是学历史的，历史是文科；所以我知道文科的导向原则——这就是说，一切形成文字的东西，都应当导向一个对我们有利的结论。我舅舅已经死了，让他死于痔疮、前列腺癌，对我们有利，就让他这样死，本无不可。但是这样一来，我就不知死在电梯里的那个老头子是谁了。他死时我已经二十岁，记得事。当时他坐电梯要到十四楼，却到了地下室，而且变得肢体残缺。有人说，那电梯是废品，每天都坏，还说管房子的收了包工头的回扣。这样说不够“导向”——这样他就是死于某个人的贪心、而不是死于制度的弊病了。必须另给他个死法。这个问题我能解决，因为我在中文系修了好几年的写作课，专门研究如何臭编的问题。</p><h1 id="Going-deeper-with-convolutions"><a href="#Going-deeper-with-convolutions" class="headerlink" title="Going deeper with convolutions"></a>Going deeper with convolutions</h1><p><strong>Christian Szegedy</strong> et al.<br><em>Published on arXiv, 2014</em> </p><p><strong>[Abstract]</strong> We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>In the last three years, mainly due to the advances of deep learning, more concretely convolutional networks [10], the quality of image recognition and object detection has been progressing at a dra- matic pace. One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detec- tion purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly more accurate. The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6].</p><p>Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms – especially their power and memory use – gains importance. It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.</p><p>In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al [12] in conjunction with the famous “we need to go deeper” internet meme [1]. In our case, the word “deep” is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the “Inception module” and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of [12] while taking inspiration and guidance from the theoretical work by Arora et al [2]. The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, on which it significantly outperforms the current state of the art.</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p>Starting with LeNet-5 [10], convolutional neural networks (CNN) have typically had a standard structure – stacked convolutional layers (optionally followed by contrast normalization and max- pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge [9, 21]. For larger datasets such as Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14], while using dropout [7] to address the problem of overfitting.</p><p>Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as [9] has also been successfully employed for localization [9, 14], object detection [6, 14, 18, 5] and human pose estimation [19]. Inspired by a neuroscience model of the primate visual cortex, Serre et al. [15] use a series of fixed Gabor filters of different sizes in order to handle multiple scales, similarly to the Inception model. However, contrary to the fixed 2-layer deep model of [15], all filters in the Inception model are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model.</p><p>Network-in-Network is an approach proposed by Lin et al. [12] in order to increase the representa- tional power of neural networks. When applied to convolutional layers, the method could be viewed as additional 1 × 1 convolutional layers followed typically by the rectified linear activation [9]. This enables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our architecture. However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks. This allows for not just increasing the depth, but also the width of our networks without significant performance penalty.</p><p>The current leading approach for object detection is the Regions with Convolutional Neural Net- works (R-CNN) proposed by Girshick et al. [6]. R-CNN decomposes the overall detection problem into two subproblems: to first utilize low-level cues such as color and superpixel consistency for potential object proposals in a category-agnostic fashion, and to then use CNN classifiers to identify object categories at those locations. Such a two stage approach leverages the accuracy of bound- ing box segmentation with low-level cues, as well as the highly powerful classification power of state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have ex- plored enhancements in both stages, such as multi-box [5] prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.</p><h2 id="3-Motivation-and-High-Level-Considerations"><a href="#3-Motivation-and-High-Level-Considerations" class="headerlink" title="3 Motivation and High Level Considerations"></a>3 Motivation and High Level Considerations</h2><p>The most straightforward way of improving the performance of deep neural networks is by increas- ing their size. This includes both increasing the depth – the number of levels – of the network and its width: the number of units at each level. This is as an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data. However this simple solution comes with two major drawbacks.</p><img src="http://cs231n.github.io/assets/trainset.jpg"><blockquote><p>Figure 1: An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category.</p></blockquote><p>Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited. This can become a major bottleneck, since the creation of high quality training sets can be tricky and expensive, especially if expert human raters are necessary to distinguish between fine-grained visual categories like those in ImageNet (even in the 1000-class ILSVRC subset) as demonstrated by <code>Figure 1</code>.</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></span><br><span class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    self.Xtr = X</span><br><span class="line">    self.ytr = y</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over all test rows</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># find the nearest training image to the i'th test image</span></span><br><span class="line">      <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">      min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE HTML&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"content-type"</span> <span class="attr">content</span>=<span class="string">"text/html;charset=utf-8;"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"X-UA-Compatible"</span> <span class="attr">content</span>=<span class="string">"IE=edge,chrome=1"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"robots"</span> <span class="attr">content</span>=<span class="string">"all"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"robots"</span> <span class="attr">content</span>=<span class="string">"index,follow"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"https://qzone.qq.com/gy/404/style/404style.css"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/plain"</span> <span class="attr">src</span>=<span class="string">"http://www.qq.com/404/search_children.js"</span></span></span><br><span class="line"><span class="tag">          <span class="attr">charset</span>=<span class="string">"utf-8"</span> <span class="attr">homePageUrl</span>=<span class="string">"/"</span></span></span><br><span class="line"><span class="tag">          <span class="attr">homePageName</span>=<span class="string">"回到我的主页"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">  </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://qzone.qq.com/gy/404/data.js"</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://qzone.qq.com/gy/404/page.js"</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Media"><a href="#Media" class="headerlink" title="Media"></a>Media</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;This-is-a-h1-title&quot;&gt;&lt;a href=&quot;#This-is-a-h1-title&quot; class=&quot;headerlink&quot; title=&quot;This is a h1 title&quot;&gt;&lt;/a&gt;This is a h1 title&lt;/h1&gt;&lt;h2 id=&quot;T
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Backpropagation for a Linear Layer (test)</title>
    <link href="http://xiekai.site/2018/02/05/test3/"/>
    <id>http://xiekai.site/2018/02/05/test3/</id>
    <published>2018-02-05T08:39:24.000Z</published>
    <updated>2018-02-05T09:18:26.785Z</updated>
    
    <content type="html"><![CDATA[<p>In these notes we will explicitly derive the equations to use when backpropagating through a linear layer, using minibatches.</p><p>During the forward pass, the linear layer takes an input $X$ of shape $N × D$ and a weight matrix $W$ of shape $D × M$, and computes an output $Y = XW$ of shape $N × M$ by computing the matrix product of the two inputs. To make things even more concrete, we will consider the case $N = 2, D = 2, M = 3$.</p><p>We can then write out the forward pass in terms of the elements of the inputs:</p><h3 id="KKT-Conditions"><a href="#KKT-Conditions" class="headerlink" title="KKT Conditions"></a>KKT Conditions</h3><p>$x$ is global solution of the CP problem</p><script type="math/tex; mode=display">min \ f(x) \\s.t. a_i(x)=0,\ for i=1,...,b \\c_j(x) \leq 0,\ for j=1,...,q</script><p>$iff.$<br>$(i)  a_i(x)=0 \quad for i=1,…,p$<br>$(ii)  a_j(x) \leq 0 \quad for j=1,…,q$<br>$(iii)  \nabla f(x)+\sum_{i=1}^{p}\lambda_i\nabla a_i(x)+\sum_{j=1}^{q}\mu_j\nabla c_j(x)=0$<br>$(iv)  \mu_j c_j(x)=0 \quad for j=1,…,q$<br>$(v)  \mu_j \geq 0 \quad for j=1,…,q$</p><h2 id="Q-k-means-clustering"><a href="#Q-k-means-clustering" class="headerlink" title="Q: k-means clustering"></a>Q: k-means clustering</h2><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul><li>$D=\{x_1,x_2,…,x_N\}, x_n\in R^d$</li><li>聚类簇数：$K$</li></ul><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ul><li>定义 $\mu_k$: 第$k$类的聚类中心，$k=1,…,K$</li><li>定义 $r_{nk}\in\{0,1\}$: 数据点$x_n$属于$K$个聚类中的哪一个 </li><li>目标函数：$J=\sum_{n=1}^{N}\sum_{k=1}^{K} r_{nk} ||x_n-\mu_k||^2$，目标：找到$\{r_{nk}\}$和$\{\mu_k\}$的值，使得$J$达到最小值</li><li>迭代<ul><li>E Step: 关于$\{r_{nk}\}$最小化$J$，保持$\{\mu_k\}$固定<br>对每个$n$分别进行最优化，只要$k$的值使$||x_n-\mu_k||^2$最小，就令$\{r_{nk}\}$等于1，即<script type="math/tex; mode=display">r_{nk}=\begin{cases} 1& if\quad k=argmin_j||x_n-\mu_k||^2\\0& otherwise \end{cases}</script></li><li>M Step: 关于$\{\mu_k\}$最小化$J$，保持$\{r_{nk}\}$固定<br>对每个$\mu_k, l\in\{1,2,…,K\}$进行最优化<script type="math/tex; mode=display">\begin{aligned} \nabla_{\mu_k}J &= \sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\nabla_{\mu_k} ||x_n-\mu_k||^2 \\&=\sum_{n=1}^{N}r_{nk}\nabla_{\mu_k} ||x_n-\mu_k||^2 \\&=2\sum_{n=1}^{N}r_{nk}(\mu_k-x_n)=0  \end{aligned}\\\Rightarrow \mu_k=\frac{\sum_{n=1}^{N}r_{nk}x_n}{\sum_{n=1}^{N}r_{nk}}</script>即令$\mu_k$等于类别k的所有数据点的均值</li><li>直到所有均值向量均为更新（或设置最大运行轮数/最小调整幅度）</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In these notes we will explicitly derive the equations to use when backpropagating through a linear layer, using minibatches.&lt;/p&gt;
&lt;p&gt;Duri
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Review of ECNU PRML Lectures (test)</title>
    <link href="http://xiekai.site/2018/02/05/test2/"/>
    <id>http://xiekai.site/2018/02/05/test2/</id>
    <published>2018-02-05T08:36:51.000Z</published>
    <updated>2018-02-05T08:50:32.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lecture-1-Introduction-amp-generic-techniques"><a href="#Lecture-1-Introduction-amp-generic-techniques" class="headerlink" title="Lecture 1: Introduction &amp; generic techniques"></a>Lecture 1: Introduction &amp; generic techniques</h2><h3 id="过拟合-Overfitting-与规范化-Regularization"><a href="#过拟合-Overfitting-与规范化-Regularization" class="headerlink" title="过拟合(Overfitting)与规范化(Regularization)"></a>过拟合(Overfitting)与规范化(Regularization)</h3><ul><li><p>过拟合<br>过拟合是指学习时选择的模型所包含的参数过多，导致对已知数据预测得很好，但对未知数据预测很差的现象。即所得的假设函数拥有较低的训练误差，但有较高的泛化误差。导致过拟合的主要影响因素有：模型的复杂度、训练数据的噪声、数据规模。过拟合无法彻底避免，只能减小其风险。</p></li><li><p>规范化<br>规范化（正则化）是一种控制过拟合现象的技术。通过给误差函数增加一个惩罚项，使得系数不会达到很大的值。这种惩罚项最简单的形式是采用所有系数的平方和的形式。<br>一般形式：</p><script type="math/tex; mode=display">\arg\min \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)</script><p>其中，第一项为经验风险，第二项为规范化项，$\lambda$调整两者之间的关系。</p></li></ul><h3 id="参数估计方法"><a href="#参数估计方法" class="headerlink" title="参数估计方法"></a>参数估计方法</h3><p>频率学派：就事件本身建模。认为参数固定，样本随机。只要样本分布确定了，参数就是个固定的值。e.g. MLE<br>贝叶斯派：参数随机，样本固定。参数服从先验分布，通过不断获取新的样本，去修正参数（即形成后验分布）e.g. MAP</p><ul><li>贝叶斯公式<script type="math/tex; mode=display">p(\theta|X)=\frac{p(X|\theta)p(\theta)}{p(X)}</script></li></ul><p>给定数据集：$X=(x_1,x_2,…,x_N)^T$</p><h4 id="最大似然估计（MLE）"><a href="#最大似然估计（MLE）" class="headerlink" title="最大似然估计（MLE）"></a>最大似然估计（MLE）</h4><p>最大似然估计就是用似然函数取到最大值时的参数值作为估计值，似然函数可以写做：</p><script type="math/tex; mode=display">L(\theta)=p(X | \theta)=\prod_{i=n}^N p(x_n | \theta)</script><p>为防止浮点数下溢与方便数值求解，通常取对数似然：</p><script type="math/tex; mode=display">LL(\theta)=\sum_{n=1}^N \ln p(x_n | \theta)</script><p>所以，$\hat{\theta}_{ML}$表示为（取平均对数似然）：</p><script type="math/tex; mode=display">\hat{\theta}_{ML}=\arg\max_\theta \frac{1}{N} \sum_{n=1}^N \ln p(x_n | \theta)</script><p>当使用数值方法求解极小值时，通常会将最大化问题转化为最小化问题，即取负对数。</p><h4 id="最大参数后验（MAP）"><a href="#最大参数后验（MAP）" class="headerlink" title="最大参数后验（MAP）"></a>最大参数后验（MAP）</h4><p>最大后验估计是根据经验数据获得对难以观察的量的点估计。最大后验加入了要估计量的先验分布。</p><script type="math/tex; mode=display">posterior \propto likelihood \times prior</script><p>取对数则有：</p><script type="math/tex; mode=display">\ln\{posterior\} \propto \ln\{likelihood\} + \ln\{prior\}</script><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><h2 id="Lecture-2-Gaussian-distribution-amp-related-techniques"><a href="#Lecture-2-Gaussian-distribution-amp-related-techniques" class="headerlink" title="Lecture 2: Gaussian distribution &amp; related techniques"></a>Lecture 2: Gaussian distribution &amp; related techniques</h2><h3 id="高斯分布的性质"><a href="#高斯分布的性质" class="headerlink" title="高斯分布的性质"></a>高斯分布的性质</h3><ul><li>一元变量$x$的情形<script type="math/tex; mode=display">\mathcal N(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\{-\frac{1}{2\sigma^2}(x-\mu)^2\}</script>其中$\mu$是均值，$\sigma^2$是方差。<ul><li>$\mathbb E[x]=\mu$</li><li>$var[x]=\sigma^2$</li></ul></li><li>多元情况<script type="math/tex; mode=display">\mathcal N(x|\mu,\Sigma)=\frac{1}{(2\pi)^\frac{D}{2}}\frac{1}{|\Sigma|^\frac{1}{2}}\exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}</script>其中，$\mu$是一个D维均值向量，$\Sigma$是一个$D\times D$的协方差矩阵，$|\Sigma|$是$\Sigma$的行列式<ul><li>$\mathbb E[\bf{x}]=\bf{\mu}$</li><li>$cov[\bf{x}]=\bf{\Sigma}$</li></ul></li><li>中心极限定理：设从均值为$\mu$，方差为$\sigma^2$（有限）的任意一个总体中抽取样本量为$n$的样本；当$n$充分大时，样本均值的抽样分布近似服从$\mu$，方差为$\frac{\sigma^2}{n}$的正态分布。</li></ul><h3 id="Woodbury矩阵求逆"><a href="#Woodbury矩阵求逆" class="headerlink" title="Woodbury矩阵求逆"></a>Woodbury矩阵求逆</h3><script type="math/tex; mode=display">(A+BCD)^{-1}=A^{-1}-A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}</script><h2 id="Lecture-3-Linear-models-for-regression"><a href="#Lecture-3-Linear-models-for-regression" class="headerlink" title="Lecture 3: Linear models for regression"></a>Lecture 3: Linear models for regression</h2><p>回归问题的目标是在给定$D$维输入（input）变量$x$的情况下，预测一个或者多个连续目标（target）变量$t$的值。<br>给定一个由$N$个观测值$\{x_n\}$组成的数据集，其中$n=1,…,N$，以及对应的目标值{t_n}，我们的目标是预测对于给定新的$x$值的情况下，$t$的值。</p><ul><li>方法：<ol><li>建立一个适当的函数$y(x)$，对于新$x$，直接给出对应的$t$值。</li><li>对预测分布$p(t | x)$建模。（等同于最小化一个恰当选择的损失函数的期望值）</li></ol></li></ul><h3 id="线性回归模型（线性基函数模型）"><a href="#线性回归模型（线性基函数模型）" class="headerlink" title="线性回归模型（线性基函数模型）"></a>线性回归模型（线性基函数模型）</h3><ul><li>线性回归模型：表示模型是关于参数的线性函数</li><li><p>线性基函数模型</p><script type="math/tex; mode=display">y(x,w)=w_0+\sum_{j=1}^{M-1}w_j \phi_j(x)</script><p>其中，$x=(x1,…,x_D)^T, w=(w0,…,w_D)^T, \phi_j(x)$为基函数（basis function），参数总数为$M$。<br>方便起见，定义$\phi_0(x)=1$：</p><script type="math/tex; mode=display">y(x,w)=\sum_{j=0}^{M-1}w_j \phi_j(x)=w^T\phi(x)</script><p>其中，$\phi=(\phi_0,…,\phi_{M-1})^T$</p></li><li><p>通过使用非线性基函数，能够让函数$y(x,w)$成为输入向量$x$的一个非线性函数。通常选择如下基函数：</p><ul><li>幂指数<script type="math/tex; mode=display">\phi_j(x)=x^j</script></li><li>高斯基函数<script type="math/tex; mode=display">\phi_j(x)=exp\{-\frac{(x-\mu_j)^2}{2s^2}\}</script>$\mu_j$控制了基函数在输入空间中的位置，参数$s$控制了基函数的空间大小</li><li>sigmoid基函数<script type="math/tex; mode=display">\phi_j(x)=\sigma(\frac{x-\mu_j}{s})</script>其中$\sigma(a)=\frac{1}{1+e^{-a}}$为logistic sigmoid</li><li>傅立叶基函数</li></ul></li></ul><h3 id="最小二乘与规范化最小二乘"><a href="#最小二乘与规范化最小二乘" class="headerlink" title="最小二乘与规范化最小二乘"></a>最小二乘与规范化最小二乘</h3><script type="math/tex; mode=display">E_D(w)=\frac{1}{2} \sum_{n=1}^N\{t_n-w^T \phi(x_n)\}^2</script><h4 id="规范化最小二乘"><a href="#规范化最小二乘" class="headerlink" title="规范化最小二乘"></a>规范化最小二乘</h4><p>总的误差函数表达为：$E_D(w)+\lambda E_W(w)$</p><ul><li><p>正则化项$E_W(w)$的选择</p><ul><li>权向量各个元素的平方和<script type="math/tex; mode=display">E_W(w)=\frac{1}{2}w^Tw</script></li><li><p>更一般化</p><script type="math/tex; mode=display">E_W(w)=\frac{1}{2}\sum_{j=1}^M |w_j|^q</script></li><li><p>$q=1$的情形称为套索（lasso），$q=2$对应于二次规范化项。</p></li></ul></li></ul><h2 id="Lecture-4-Essential-numerical-mathematics-amp-vector-calculus"><a href="#Lecture-4-Essential-numerical-mathematics-amp-vector-calculus" class="headerlink" title="Lecture 4: Essential numerical mathematics &amp; vector calculus"></a>Lecture 4: Essential numerical mathematics &amp; vector calculus</h2><h3 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h3><h3 id="共轭梯度算法"><a href="#共轭梯度算法" class="headerlink" title="共轭梯度算法"></a>共轭梯度算法</h3><h3 id="向量微积分"><a href="#向量微积分" class="headerlink" title="向量微积分"></a>向量微积分</h3><h2 id="Lecture-5-Linear-models-for-classification"><a href="#Lecture-5-Linear-models-for-classification" class="headerlink" title="Lecture 5: Linear models for classification"></a>Lecture 5: Linear models for classification</h2><p>分类的目标是将输入变量$x$分到K个离散的类别$C_k$中的某一类。</p><ul><li>分类线性模型：决策面是$x$的线性函数<h3 id="判别函数、概率产生式模型、概率判别式模型"><a href="#判别函数、概率产生式模型、概率判别式模型" class="headerlink" title="判别函数、概率产生式模型、概率判别式模型"></a>判别函数、概率产生式模型、概率判别式模型</h3>对于分类问题：可分为推断（inference）和决策（decision）两个阶段。推断阶段，使用训练数据学习$p(C_k|x)$的模型；决策阶段，使用后验概率来进行最优的分类。也可以同时解决两个问题，即简单地学习一个函数（判别函数，discriminant function），将输入$x$直接映射为决策。</li><li>三种方法：</li></ul><ol><li>判别函数（discriminative function）<br>把每个输入$x$直接映射为类别标签（不接触后验概率）</li><li>概率判别式模型（discriminative model）<br>直接对后验概率分布$p(C_k | x)$建模，例如把条件概率分布表示为参数模型，然后使用训练集来最优化参数。</li><li>概率生成式模型（generative model）<br>对类条件概率密度$p(x | C_k)$以及类的先验概率分布$p(C_k)$建模，然后我们使用贝叶斯定理计算后验概率分布$p(C_k | x)$</li></ol><h3 id="线性判别函数"><a href="#线性判别函数" class="headerlink" title="线性判别函数"></a>线性判别函数</h3><ul><li><p>二分类</p><script type="math/tex; mode=display">y(x)=w^Tx+w_0</script><p>对于一个输入向量$x$，如果$y(x) \leq 0$，则分到$C_1$，否则分到$C_2$。</p></li><li><p>K分类<br>K分类由K个线性判别函数组成，形式为：</p><script type="math/tex; mode=display">y_k(x)=w_k^Tx+w_{k0}</script><p>对于一个输入向量$x$，如果对于所有的$j \neq k$都有$y_k(x)&gt;y_j(x)$，那么就把它分到$C_k$。</p></li><li><p>学习参数的方法</p><ul><li><p>最小平方法：使模型的预测尽可能与目标值接近。缺点：对于离群点缺乏鲁棒性</p></li><li><p>Fisher线性判别</p><p>基本思想：从维度降低的角度看线性分类模型。目标是使输出空间的类别有最大的区分度。通过调整权向量，使得类内的投影点尽可能接近，类间的投影点尽可能远离。<br>二分类情况：$C_1$类的点有$N_1$个，$C_2$类的点有$N_2$个<br>两类的均值向量为：$m_1=\frac{1}{N_1}\sum_{n \in C_1}x_n, m_2=\frac{1}{N_2}\sum_{n \in C_2}x_n$</p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lecture-1-Introduction-amp-generic-techniques&quot;&gt;&lt;a href=&quot;#Lecture-1-Introduction-amp-generic-techniques&quot; class=&quot;headerlink&quot; title=&quot;Le
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Image Classification, kNN, SVM, Softmax, Neural Network (test)</title>
    <link href="http://xiekai.site/2018/01/31/test/"/>
    <id>http://xiekai.site/2018/01/31/test/</id>
    <published>2018-01-31T03:27:04.000Z</published>
    <updated>2018-02-05T14:30:30.511Z</updated>
    
    <content type="html"><![CDATA[<p>In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier. The goals of this assignment are as follows:</p><ul><li>understand the basic <strong>Image Classification pipeline</strong> and the data-driven approach (train/predict stages)</li><li>understand the train/val/test <strong>splits</strong> and the use of validation data for <strong>hyperparameter tuning</strong>.</li><li>develop proficiency in writing efficient <strong>vectorized</strong> code with numpy</li><li>implement and apply a k-Nearest Neighbor (<strong>kNN</strong>) classifier</li><li>implement and apply a Multiclass Support Vector Machine (<strong>SVM</strong>) classifier</li><li>implement and apply a <strong>Softmax</strong> classifier</li><li>implement and apply a <strong>Two layer neural network</strong> classifier</li><li>understand the differences and tradeoffs between these classifiers</li><li>get a basic understanding of performance improvements from using <strong>higher-level representations</strong> than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)</li></ul><a id="more"></a><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><p>You can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine on Google Cloud.</p><h3 id="Working-remotely-on-Google-Cloud-Recommended"><a href="#Working-remotely-on-Google-Cloud-Recommended" class="headerlink" title="Working remotely on Google Cloud (Recommended)"></a>Working remotely on Google Cloud (Recommended)</h3><p><strong>Note:</strong> after following these instructions, make sure you go to <strong>Download data</strong> below (you can skip the <strong>Working locally</strong> section).</p><p>As part of this course, you can use Google Cloud for your assignments. We recommend this route for anyone who is having trouble with installation set-up, or if you would like to use better CPU/GPU resources than you may have locally. Please see the set-up tutorial <a href="http://cs231n.github.io/gce-tutorial/" target="_blank" rel="noopener">here</a> for more details. :)</p><h3 id="Working-locally"><a href="#Working-locally" class="headerlink" title="Working locally"></a>Working locally</h3><p>Get the code as a zip file <a href="http://cs231n.stanford.edu/assignments/2017/spring1617_assignment1.zip" target="_blank" rel="noopener">here</a>. As for the dependencies:</p><p><strong>Installing Python 3.5+:</strong> To use python3, make sure to install version 3.5 or 3.6 on your local machine. If you are on Mac OS X, you can do this using Homebrew with <code>brew install python3</code>. You can find instructions for Ubuntu <a href="https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-ubuntu-16-04" target="_blank" rel="noopener">here</a>.</p><p><strong>Virtual environment:</strong> If you decide to work locally, we recommend using <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/" target="_blank" rel="noopener">virtual environment</a> for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd assignment1</span><br><span class="line">sudo pip install virtualenv      <span class="comment"># This may already be installed</span></span><br><span class="line">virtualenv -p python3 .env       <span class="comment"># Create a virtual environment (python3)</span></span><br><span class="line"><span class="comment"># Note: you can also use "virtualenv .env" to use your default python (usually python 2.7)</span></span><br><span class="line">source .env/bin/activate         <span class="comment"># Activate the virtual environment</span></span><br><span class="line">pip install -r requirements.txt  <span class="comment"># Install dependencies</span></span><br><span class="line"><span class="comment"># Work on the assignment for a while ...</span></span><br><span class="line">deactivate                       <span class="comment"># Exit the virtual environment</span></span><br></pre></td></tr></table></figure><p>Note that every time you want to work on the assignment, you should run <code>source .env/bin/activate</code> (from within your <code>assignment1</code> folder) to re-activate the virtual environment, and <code>deactivate</code> again whenever you are done.</p><h3 id="Download-data"><a href="#Download-data" class="headerlink" title="Download data:"></a>Download data:</h3><p>Once you have the starter code (regardless of which method you choose above), you will need to download the CIFAR-10 dataset. Run the following from the <code>assignment1</code> directory:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd cs231n/datasets</span><br><span class="line">./get_datasets.sh</span><br></pre></td></tr></table></figure><h3 id="Start-IPython"><a href="#Start-IPython" class="headerlink" title="Start IPython:"></a>Start IPython:</h3><p>After you have the CIFAR-10 data, you should start the IPython notebook server from the <code>assignment1</code> directory, with the <code>jupyter notebook</code> command. (See the <a href="http://cs231n.github.io/gce-tutorial/" target="_blank" rel="noopener">Google Cloud Tutorial</a> for any additional steps you may need to do for setting this up, if you are working remotely)</p><p>If you are unfamiliar with IPython, you can also refer to our <a href="http://cs231n.github.io/ipython-tutorial" target="_blank" rel="noopener">IPython tutorial</a>.</p><h3 id="Some-Notes"><a href="#Some-Notes" class="headerlink" title="Some Notes"></a>Some Notes</h3><p><strong>NOTE 1:</strong> This year, the <code>assignment1</code> code has been tested to be compatible with python versions <code>2.7</code>, <code>3.5</code>, <code>3.6</code> (it may work with other versions of <code>3.x</code>, but we won’t be officially supporting them). You will need to make sure that during your <code>virtualenv</code> setup that the correct version of <code>python</code> is used. You can confirm your python version by (1) activating your virtualenv and (2) running <code>which python</code>.</p><p><strong>NOTE 2:</strong> If you are working in a virtual environment on OSX, you may <em>potentially</em> encounter errors with matplotlib due to the <a href="http://matplotlib.org/faq/virtualenv_faq.html" target="_blank" rel="noopener">issues described here</a>. In our testing, it seems that this issue is no longer present with the most recent version of matplotlib, but if you do end up running into this issue you may have to use the <code>start_ipython_osx.sh</code> script from the <code>assignment1</code> directory (instead of <code>jupyter notebook</code> above) to launch your IPython notebook server. Note that you may have to modify some variables within the script to match your version of python/installation directory. The script assumes that your virtual environment is named <code>.env</code>.</p><h3 id="Submitting-your-work"><a href="#Submitting-your-work" class="headerlink" title="Submitting your work:"></a>Submitting your work:</h3><p>Whether you work on the assignment locally or using Google Cloud, once you are done working run the <code>collectSubmission.sh</code> script; this will produce a file called <code>assignment1.zip</code>. Please submit this file on <a href="https://canvas.stanford.edu/courses/66461/" target="_blank" rel="noopener">Canvas</a>.</p><h3 id="Q1-k-Nearest-Neighbor-classifier-20-points"><a href="#Q1-k-Nearest-Neighbor-classifier-20-points" class="headerlink" title="Q1: k-Nearest Neighbor classifier (20 points)"></a>Q1: k-Nearest Neighbor classifier (20 points)</h3><p>The IPython Notebook <strong>knn.ipynb</strong> will walk you through implementing the kNN classifier.</p><h3 id="Q2-Training-a-Support-Vector-Machine-25-points"><a href="#Q2-Training-a-Support-Vector-Machine-25-points" class="headerlink" title="Q2: Training a Support Vector Machine (25 points)"></a>Q2: Training a Support Vector Machine (25 points)</h3><p>The IPython Notebook <strong>svm.ipynb</strong> will walk you through implementing the SVM classifier.</p><h3 id="Q3-Implement-a-Softmax-classifier-20-points"><a href="#Q3-Implement-a-Softmax-classifier-20-points" class="headerlink" title="Q3: Implement a Softmax classifier (20 points)"></a>Q3: Implement a Softmax classifier (20 points)</h3><p>The IPython Notebook <strong>softmax.ipynb</strong> will walk you through implementing the Softmax classifier.</p><h3 id="Q4-Two-Layer-Neural-Network-25-points"><a href="#Q4-Two-Layer-Neural-Network-25-points" class="headerlink" title="Q4: Two-Layer Neural Network (25 points)"></a>Q4: Two-Layer Neural Network (25 points)</h3><p>The IPython Notebook <strong>two_layer_net.ipynb</strong> will walk you through the implementation of a two-layer neural network classifier.</p><h3 id="Q5-Higher-Level-Representations-Image-Features-10-points"><a href="#Q5-Higher-Level-Representations-Image-Features-10-points" class="headerlink" title="Q5: Higher Level Representations: Image Features (10 points)"></a>Q5: Higher Level Representations: Image Features (10 points)</h3><p>The IPython Notebook <strong>features.ipynb</strong> will walk you through this exercise, in which you will examine the improvements gained by using higher-level representations as opposed to using raw pixel values.</p><h3 id="Q6-Cool-Bonus-Do-something-extra-10-points"><a href="#Q6-Cool-Bonus-Do-something-extra-10-points" class="headerlink" title="Q6: Cool Bonus: Do something extra! (+10 points)"></a>Q6: Cool Bonus: Do something extra! (+10 points)</h3><p>Implement, investigate or analyze something extra surrounding the topics in this assignment, and using the code you developed. For example, is there some other interesting question we could have asked? Is there any insightful visualization you can plot? Or anything fun to look at? Or maybe you can experiment with a spin on the loss function? If you try out something cool we’ll give you up to 10 extra points and may feature your results in the lecture.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier. The goals of this assignment are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;understand the basic &lt;strong&gt;Image Classification pipeline&lt;/strong&gt; and the data-driven approach (train/predict stages)&lt;/li&gt;
&lt;li&gt;understand the train/val/test &lt;strong&gt;splits&lt;/strong&gt; and the use of validation data for &lt;strong&gt;hyperparameter tuning&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;develop proficiency in writing efficient &lt;strong&gt;vectorized&lt;/strong&gt; code with numpy&lt;/li&gt;
&lt;li&gt;implement and apply a k-Nearest Neighbor (&lt;strong&gt;kNN&lt;/strong&gt;) classifier&lt;/li&gt;
&lt;li&gt;implement and apply a Multiclass Support Vector Machine (&lt;strong&gt;SVM&lt;/strong&gt;) classifier&lt;/li&gt;
&lt;li&gt;implement and apply a &lt;strong&gt;Softmax&lt;/strong&gt; classifier&lt;/li&gt;
&lt;li&gt;implement and apply a &lt;strong&gt;Two layer neural network&lt;/strong&gt; classifier&lt;/li&gt;
&lt;li&gt;understand the differences and tradeoffs between these classifiers&lt;/li&gt;
&lt;li&gt;get a basic understanding of performance improvements from using &lt;strong&gt;higher-level representations&lt;/strong&gt; than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="paper" scheme="http://xiekai.site/categories/paper/"/>
    
    
      <category term="tag1" scheme="http://xiekai.site/tags/tag1/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://xiekai.site/2018/01/31/hello-world/"/>
    <id>http://xiekai.site/2018/01/31/hello-world/</id>
    <published>2018-01-31T00:53:23.954Z</published>
    <updated>2018-02-05T06:18:37.166Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h1 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h1><h4 id="Show-Some-Python-Code"><a href="#Show-Some-Python-Code" class="headerlink" title="Show Some Python Code"></a>Show Some Python Code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">D = np.random.randn(<span class="number">1000</span>,<span class="number">500</span>)</span><br><span class="line">hidden_layer_sizes = [<span class="number">500</span>]*<span class="number">10</span></span><br><span class="line">nonlinearities = [<span class="string">'tanh'</span>]*len(hidden_layer_sizes)</span><br><span class="line"></span><br><span class="line">act = &#123;<span class="string">'relu'</span>:<span class="keyword">lambda</span> x:np.maximum(<span class="number">0</span>,x), <span class="string">'tanh'</span>:<span class="keyword">lambda</span> x:np.tanh(x)&#125;</span><br><span class="line">Hs = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(hidden_layer_sizes)):</span><br><span class="line">    X = D <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> Hs[i<span class="number">-1</span>] <span class="comment"># input at this layer</span></span><br><span class="line">    fan_in = X.shape[<span class="number">1</span>]</span><br><span class="line">    fan_out = hidden_layer_sizes[i]</span><br><span class="line">    W = np.random.randn(fan_in, fan_out) * <span class="number">0.01</span> <span class="comment"># layer init</span></span><br><span class="line"></span><br><span class="line">    H = np.dot(X, W) <span class="comment"># matrix multiply</span></span><br><span class="line">    H = act[nonlinearities[i]](H) <span class="comment"># nonlinearity</span></span><br><span class="line">    Hs[i] = H <span class="comment"># cache result on this layer</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h4 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h4 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h4 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="essay" scheme="http://xiekai.site/categories/essay/"/>
    
    
  </entry>
  
</feed>
