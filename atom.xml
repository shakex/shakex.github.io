<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shake and Shake&#39;s Vision</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xiekai.site/"/>
  <updated>2018-02-05T09:18:26.785Z</updated>
  <id>http://xiekai.site/</id>
  
  <author>
    <name>Shake</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Backpropagation for a Linear Layer (test)</title>
    <link href="http://xiekai.site/2018/02/05/test3/"/>
    <id>http://xiekai.site/2018/02/05/test3/</id>
    <published>2018-02-05T08:39:24.000Z</published>
    <updated>2018-02-05T09:18:26.785Z</updated>
    
    <content type="html"><![CDATA[<p>In these notes we will explicitly derive the equations to use when backpropagating through a linear layer, using minibatches.</p><p>During the forward pass, the linear layer takes an input $X$ of shape $N × D$ and a weight matrix $W$ of shape $D × M$, and computes an output $Y = XW$ of shape $N × M$ by computing the matrix product of the two inputs. To make things even more concrete, we will consider the case $N = 2, D = 2, M = 3$.</p><p>We can then write out the forward pass in terms of the elements of the inputs:</p><h3 id="KKT-Conditions"><a href="#KKT-Conditions" class="headerlink" title="KKT Conditions"></a>KKT Conditions</h3><p>$x$ is global solution of the CP problem</p><script type="math/tex; mode=display">min \ f(x) \\s.t. a_i(x)=0,\ for i=1,...,b \\c_j(x) \leq 0,\ for j=1,...,q</script><p>$iff.$<br>$(i)  a_i(x)=0 \quad for i=1,…,p$<br>$(ii)  a_j(x) \leq 0 \quad for j=1,…,q$<br>$(iii)  \nabla f(x)+\sum_{i=1}^{p}\lambda_i\nabla a_i(x)+\sum_{j=1}^{q}\mu_j\nabla c_j(x)=0$<br>$(iv)  \mu_j c_j(x)=0 \quad for j=1,…,q$<br>$(v)  \mu_j \geq 0 \quad for j=1,…,q$</p><h2 id="Q-k-means-clustering"><a href="#Q-k-means-clustering" class="headerlink" title="Q: k-means clustering"></a>Q: k-means clustering</h2><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul><li>$D=\{x_1,x_2,…,x_N\}, x_n\in R^d$</li><li>聚类簇数：$K$</li></ul><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ul><li>定义 $\mu_k$: 第$k$类的聚类中心，$k=1,…,K$</li><li>定义 $r_{nk}\in\{0,1\}$: 数据点$x_n$属于$K$个聚类中的哪一个 </li><li>目标函数：$J=\sum_{n=1}^{N}\sum_{k=1}^{K} r_{nk} ||x_n-\mu_k||^2$，目标：找到$\{r_{nk}\}$和$\{\mu_k\}$的值，使得$J$达到最小值</li><li>迭代<ul><li>E Step: 关于$\{r_{nk}\}$最小化$J$，保持$\{\mu_k\}$固定<br>对每个$n$分别进行最优化，只要$k$的值使$||x_n-\mu_k||^2$最小，就令$\{r_{nk}\}$等于1，即<script type="math/tex; mode=display">r_{nk}=\begin{cases} 1& if\quad k=argmin_j||x_n-\mu_k||^2\\0& otherwise \end{cases}</script></li><li>M Step: 关于$\{\mu_k\}$最小化$J$，保持$\{r_{nk}\}$固定<br>对每个$\mu_k, l\in\{1,2,…,K\}$进行最优化<script type="math/tex; mode=display">\begin{aligned} \nabla_{\mu_k}J &= \sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\nabla_{\mu_k} ||x_n-\mu_k||^2 \\&=\sum_{n=1}^{N}r_{nk}\nabla_{\mu_k} ||x_n-\mu_k||^2 \\&=2\sum_{n=1}^{N}r_{nk}(\mu_k-x_n)=0  \end{aligned}\\\Rightarrow \mu_k=\frac{\sum_{n=1}^{N}r_{nk}x_n}{\sum_{n=1}^{N}r_{nk}}</script>即令$\mu_k$等于类别k的所有数据点的均值</li><li>直到所有均值向量均为更新（或设置最大运行轮数/最小调整幅度）</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In these notes we will explicitly derive the equations to use when backpropagating through a linear layer, using minibatches.&lt;/p&gt;
&lt;p&gt;Duri
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Review of ECNU PRML Lectures (test)</title>
    <link href="http://xiekai.site/2018/02/05/test2/"/>
    <id>http://xiekai.site/2018/02/05/test2/</id>
    <published>2018-02-05T08:36:51.000Z</published>
    <updated>2018-02-05T08:50:32.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lecture-1-Introduction-amp-generic-techniques"><a href="#Lecture-1-Introduction-amp-generic-techniques" class="headerlink" title="Lecture 1: Introduction &amp; generic techniques"></a>Lecture 1: Introduction &amp; generic techniques</h2><h3 id="过拟合-Overfitting-与规范化-Regularization"><a href="#过拟合-Overfitting-与规范化-Regularization" class="headerlink" title="过拟合(Overfitting)与规范化(Regularization)"></a>过拟合(Overfitting)与规范化(Regularization)</h3><ul><li><p>过拟合<br>过拟合是指学习时选择的模型所包含的参数过多，导致对已知数据预测得很好，但对未知数据预测很差的现象。即所得的假设函数拥有较低的训练误差，但有较高的泛化误差。导致过拟合的主要影响因素有：模型的复杂度、训练数据的噪声、数据规模。过拟合无法彻底避免，只能减小其风险。</p></li><li><p>规范化<br>规范化（正则化）是一种控制过拟合现象的技术。通过给误差函数增加一个惩罚项，使得系数不会达到很大的值。这种惩罚项最简单的形式是采用所有系数的平方和的形式。<br>一般形式：</p><script type="math/tex; mode=display">\arg\min \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)</script><p>其中，第一项为经验风险，第二项为规范化项，$\lambda$调整两者之间的关系。</p></li></ul><h3 id="参数估计方法"><a href="#参数估计方法" class="headerlink" title="参数估计方法"></a>参数估计方法</h3><p>频率学派：就事件本身建模。认为参数固定，样本随机。只要样本分布确定了，参数就是个固定的值。e.g. MLE<br>贝叶斯派：参数随机，样本固定。参数服从先验分布，通过不断获取新的样本，去修正参数（即形成后验分布）e.g. MAP</p><ul><li>贝叶斯公式<script type="math/tex; mode=display">p(\theta|X)=\frac{p(X|\theta)p(\theta)}{p(X)}</script></li></ul><p>给定数据集：$X=(x_1,x_2,…,x_N)^T$</p><h4 id="最大似然估计（MLE）"><a href="#最大似然估计（MLE）" class="headerlink" title="最大似然估计（MLE）"></a>最大似然估计（MLE）</h4><p>最大似然估计就是用似然函数取到最大值时的参数值作为估计值，似然函数可以写做：</p><script type="math/tex; mode=display">L(\theta)=p(X | \theta)=\prod_{i=n}^N p(x_n | \theta)</script><p>为防止浮点数下溢与方便数值求解，通常取对数似然：</p><script type="math/tex; mode=display">LL(\theta)=\sum_{n=1}^N \ln p(x_n | \theta)</script><p>所以，$\hat{\theta}_{ML}$表示为（取平均对数似然）：</p><script type="math/tex; mode=display">\hat{\theta}_{ML}=\arg\max_\theta \frac{1}{N} \sum_{n=1}^N \ln p(x_n | \theta)</script><p>当使用数值方法求解极小值时，通常会将最大化问题转化为最小化问题，即取负对数。</p><h4 id="最大参数后验（MAP）"><a href="#最大参数后验（MAP）" class="headerlink" title="最大参数后验（MAP）"></a>最大参数后验（MAP）</h4><p>最大后验估计是根据经验数据获得对难以观察的量的点估计。最大后验加入了要估计量的先验分布。</p><script type="math/tex; mode=display">posterior \propto likelihood \times prior</script><p>取对数则有：</p><script type="math/tex; mode=display">\ln\{posterior\} \propto \ln\{likelihood\} + \ln\{prior\}</script><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><h2 id="Lecture-2-Gaussian-distribution-amp-related-techniques"><a href="#Lecture-2-Gaussian-distribution-amp-related-techniques" class="headerlink" title="Lecture 2: Gaussian distribution &amp; related techniques"></a>Lecture 2: Gaussian distribution &amp; related techniques</h2><h3 id="高斯分布的性质"><a href="#高斯分布的性质" class="headerlink" title="高斯分布的性质"></a>高斯分布的性质</h3><ul><li>一元变量$x$的情形<script type="math/tex; mode=display">\mathcal N(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\{-\frac{1}{2\sigma^2}(x-\mu)^2\}</script>其中$\mu$是均值，$\sigma^2$是方差。<ul><li>$\mathbb E[x]=\mu$</li><li>$var[x]=\sigma^2$</li></ul></li><li>多元情况<script type="math/tex; mode=display">\mathcal N(x|\mu,\Sigma)=\frac{1}{(2\pi)^\frac{D}{2}}\frac{1}{|\Sigma|^\frac{1}{2}}\exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}</script>其中，$\mu$是一个D维均值向量，$\Sigma$是一个$D\times D$的协方差矩阵，$|\Sigma|$是$\Sigma$的行列式<ul><li>$\mathbb E[\bf{x}]=\bf{\mu}$</li><li>$cov[\bf{x}]=\bf{\Sigma}$</li></ul></li><li>中心极限定理：设从均值为$\mu$，方差为$\sigma^2$（有限）的任意一个总体中抽取样本量为$n$的样本；当$n$充分大时，样本均值的抽样分布近似服从$\mu$，方差为$\frac{\sigma^2}{n}$的正态分布。</li></ul><h3 id="Woodbury矩阵求逆"><a href="#Woodbury矩阵求逆" class="headerlink" title="Woodbury矩阵求逆"></a>Woodbury矩阵求逆</h3><script type="math/tex; mode=display">(A+BCD)^{-1}=A^{-1}-A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}</script><h2 id="Lecture-3-Linear-models-for-regression"><a href="#Lecture-3-Linear-models-for-regression" class="headerlink" title="Lecture 3: Linear models for regression"></a>Lecture 3: Linear models for regression</h2><p>回归问题的目标是在给定$D$维输入（input）变量$x$的情况下，预测一个或者多个连续目标（target）变量$t$的值。<br>给定一个由$N$个观测值$\{x_n\}$组成的数据集，其中$n=1,…,N$，以及对应的目标值{t_n}，我们的目标是预测对于给定新的$x$值的情况下，$t$的值。</p><ul><li>方法：<ol><li>建立一个适当的函数$y(x)$，对于新$x$，直接给出对应的$t$值。</li><li>对预测分布$p(t | x)$建模。（等同于最小化一个恰当选择的损失函数的期望值）</li></ol></li></ul><h3 id="线性回归模型（线性基函数模型）"><a href="#线性回归模型（线性基函数模型）" class="headerlink" title="线性回归模型（线性基函数模型）"></a>线性回归模型（线性基函数模型）</h3><ul><li>线性回归模型：表示模型是关于参数的线性函数</li><li><p>线性基函数模型</p><script type="math/tex; mode=display">y(x,w)=w_0+\sum_{j=1}^{M-1}w_j \phi_j(x)</script><p>其中，$x=(x1,…,x_D)^T, w=(w0,…,w_D)^T, \phi_j(x)$为基函数（basis function），参数总数为$M$。<br>方便起见，定义$\phi_0(x)=1$：</p><script type="math/tex; mode=display">y(x,w)=\sum_{j=0}^{M-1}w_j \phi_j(x)=w^T\phi(x)</script><p>其中，$\phi=(\phi_0,…,\phi_{M-1})^T$</p></li><li><p>通过使用非线性基函数，能够让函数$y(x,w)$成为输入向量$x$的一个非线性函数。通常选择如下基函数：</p><ul><li>幂指数<script type="math/tex; mode=display">\phi_j(x)=x^j</script></li><li>高斯基函数<script type="math/tex; mode=display">\phi_j(x)=exp\{-\frac{(x-\mu_j)^2}{2s^2}\}</script>$\mu_j$控制了基函数在输入空间中的位置，参数$s$控制了基函数的空间大小</li><li>sigmoid基函数<script type="math/tex; mode=display">\phi_j(x)=\sigma(\frac{x-\mu_j}{s})</script>其中$\sigma(a)=\frac{1}{1+e^{-a}}$为logistic sigmoid</li><li>傅立叶基函数</li></ul></li></ul><h3 id="最小二乘与规范化最小二乘"><a href="#最小二乘与规范化最小二乘" class="headerlink" title="最小二乘与规范化最小二乘"></a>最小二乘与规范化最小二乘</h3><script type="math/tex; mode=display">E_D(w)=\frac{1}{2} \sum_{n=1}^N\{t_n-w^T \phi(x_n)\}^2</script><h4 id="规范化最小二乘"><a href="#规范化最小二乘" class="headerlink" title="规范化最小二乘"></a>规范化最小二乘</h4><p>总的误差函数表达为：$E_D(w)+\lambda E_W(w)$</p><ul><li><p>正则化项$E_W(w)$的选择</p><ul><li>权向量各个元素的平方和<script type="math/tex; mode=display">E_W(w)=\frac{1}{2}w^Tw</script></li><li><p>更一般化</p><script type="math/tex; mode=display">E_W(w)=\frac{1}{2}\sum_{j=1}^M |w_j|^q</script></li><li><p>$q=1$的情形称为套索（lasso），$q=2$对应于二次规范化项。</p></li></ul></li></ul><h2 id="Lecture-4-Essential-numerical-mathematics-amp-vector-calculus"><a href="#Lecture-4-Essential-numerical-mathematics-amp-vector-calculus" class="headerlink" title="Lecture 4: Essential numerical mathematics &amp; vector calculus"></a>Lecture 4: Essential numerical mathematics &amp; vector calculus</h2><h3 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h3><h3 id="共轭梯度算法"><a href="#共轭梯度算法" class="headerlink" title="共轭梯度算法"></a>共轭梯度算法</h3><h3 id="向量微积分"><a href="#向量微积分" class="headerlink" title="向量微积分"></a>向量微积分</h3><h2 id="Lecture-5-Linear-models-for-classification"><a href="#Lecture-5-Linear-models-for-classification" class="headerlink" title="Lecture 5: Linear models for classification"></a>Lecture 5: Linear models for classification</h2><p>分类的目标是将输入变量$x$分到K个离散的类别$C_k$中的某一类。</p><ul><li>分类线性模型：决策面是$x$的线性函数<h3 id="判别函数、概率产生式模型、概率判别式模型"><a href="#判别函数、概率产生式模型、概率判别式模型" class="headerlink" title="判别函数、概率产生式模型、概率判别式模型"></a>判别函数、概率产生式模型、概率判别式模型</h3>对于分类问题：可分为推断（inference）和决策（decision）两个阶段。推断阶段，使用训练数据学习$p(C_k|x)$的模型；决策阶段，使用后验概率来进行最优的分类。也可以同时解决两个问题，即简单地学习一个函数（判别函数，discriminant function），将输入$x$直接映射为决策。</li><li>三种方法：</li></ul><ol><li>判别函数（discriminative function）<br>把每个输入$x$直接映射为类别标签（不接触后验概率）</li><li>概率判别式模型（discriminative model）<br>直接对后验概率分布$p(C_k | x)$建模，例如把条件概率分布表示为参数模型，然后使用训练集来最优化参数。</li><li>概率生成式模型（generative model）<br>对类条件概率密度$p(x | C_k)$以及类的先验概率分布$p(C_k)$建模，然后我们使用贝叶斯定理计算后验概率分布$p(C_k | x)$</li></ol><h3 id="线性判别函数"><a href="#线性判别函数" class="headerlink" title="线性判别函数"></a>线性判别函数</h3><ul><li><p>二分类</p><script type="math/tex; mode=display">y(x)=w^Tx+w_0</script><p>对于一个输入向量$x$，如果$y(x) \leq 0$，则分到$C_1$，否则分到$C_2$。</p></li><li><p>K分类<br>K分类由K个线性判别函数组成，形式为：</p><script type="math/tex; mode=display">y_k(x)=w_k^Tx+w_{k0}</script><p>对于一个输入向量$x$，如果对于所有的$j \neq k$都有$y_k(x)&gt;y_j(x)$，那么就把它分到$C_k$。</p></li><li><p>学习参数的方法</p><ul><li><p>最小平方法：使模型的预测尽可能与目标值接近。缺点：对于离群点缺乏鲁棒性</p></li><li><p>Fisher线性判别</p><p>基本思想：从维度降低的角度看线性分类模型。目标是使输出空间的类别有最大的区分度。通过调整权向量，使得类内的投影点尽可能接近，类间的投影点尽可能远离。<br>二分类情况：$C_1$类的点有$N_1$个，$C_2$类的点有$N_2$个<br>两类的均值向量为：$m_1=\frac{1}{N_1}\sum_{n \in C_1}x_n, m_2=\frac{1}{N_2}\sum_{n \in C_2}x_n$</p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lecture-1-Introduction-amp-generic-techniques&quot;&gt;&lt;a href=&quot;#Lecture-1-Introduction-amp-generic-techniques&quot; class=&quot;headerlink&quot; title=&quot;Le
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Image Classification, kNN, SVM, Softmax, Neural Network (test)</title>
    <link href="http://xiekai.site/2018/01/31/test/"/>
    <id>http://xiekai.site/2018/01/31/test/</id>
    <published>2018-01-31T03:27:04.000Z</published>
    <updated>2018-02-05T10:23:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier. The goals of this assignment are as follows:</p><ul><li>understand the basic <strong>Image Classification pipeline</strong> and the data-driven approach (train/predict stages)</li><li>understand the train/val/test <strong>splits</strong> and the use of validation data for <strong>hyperparameter tuning</strong>.</li><li>develop proficiency in writing efficient <strong>vectorized</strong> code with numpy</li><li>implement and apply a k-Nearest Neighbor (<strong>kNN</strong>) classifier</li><li>implement and apply a Multiclass Support Vector Machine (<strong>SVM</strong>) classifier</li><li>implement and apply a <strong>Softmax</strong> classifier</li><li>implement and apply a <strong>Two layer neural network</strong> classifier</li><li>understand the differences and tradeoffs between these classifiers</li><li>get a basic understanding of performance improvements from using <strong>higher-level representations</strong> than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)</li></ul><a id="more"></a><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><p>You can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine on Google Cloud.</p><h3 id="Working-remotely-on-Google-Cloud-Recommended"><a href="#Working-remotely-on-Google-Cloud-Recommended" class="headerlink" title="Working remotely on Google Cloud (Recommended)"></a>Working remotely on Google Cloud (Recommended)</h3><p><strong>Note:</strong> after following these instructions, make sure you go to <strong>Download data</strong> below (you can skip the <strong>Working locally</strong> section).</p><p>As part of this course, you can use Google Cloud for your assignments. We recommend this route for anyone who is having trouble with installation set-up, or if you would like to use better CPU/GPU resources than you may have locally. Please see the set-up tutorial <a href="http://cs231n.github.io/gce-tutorial/" target="_blank" rel="noopener">here</a> for more details. :)</p><h3 id="Working-locally"><a href="#Working-locally" class="headerlink" title="Working locally"></a>Working locally</h3><p>Get the code as a zip file <a href="http://cs231n.stanford.edu/assignments/2017/spring1617_assignment1.zip" target="_blank" rel="noopener">here</a>. As for the dependencies:</p><p><strong>Installing Python 3.5+:</strong> To use python3, make sure to install version 3.5 or 3.6 on your local machine. If you are on Mac OS X, you can do this using Homebrew with <code>brew install python3</code>. You can find instructions for Ubuntu <a href="https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-ubuntu-16-04" target="_blank" rel="noopener">here</a>.</p><p><strong>Virtual environment:</strong> If you decide to work locally, we recommend using <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/" target="_blank" rel="noopener">virtual environment</a> for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd assignment1</span><br><span class="line">sudo pip install virtualenv      <span class="comment"># This may already be installed</span></span><br><span class="line">virtualenv -p python3 .env       <span class="comment"># Create a virtual environment (python3)</span></span><br><span class="line"><span class="comment"># Note: you can also use "virtualenv .env" to use your default python (usually python 2.7)</span></span><br><span class="line">source .env/bin/activate         <span class="comment"># Activate the virtual environment</span></span><br><span class="line">pip install -r requirements.txt  <span class="comment"># Install dependencies</span></span><br><span class="line"><span class="comment"># Work on the assignment for a while ...</span></span><br><span class="line">deactivate                       <span class="comment"># Exit the virtual environment</span></span><br></pre></td></tr></table></figure><p>Note that every time you want to work on the assignment, you should run <code>source .env/bin/activate</code> (from within your <code>assignment1</code> folder) to re-activate the virtual environment, and <code>deactivate</code> again whenever you are done.</p><h3 id="Download-data"><a href="#Download-data" class="headerlink" title="Download data:"></a>Download data:</h3><p>Once you have the starter code (regardless of which method you choose above), you will need to download the CIFAR-10 dataset. Run the following from the <code>assignment1</code> directory:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd cs231n/datasets</span><br><span class="line">./get_datasets.sh</span><br></pre></td></tr></table></figure><h3 id="Start-IPython"><a href="#Start-IPython" class="headerlink" title="Start IPython:"></a>Start IPython:</h3><p>After you have the CIFAR-10 data, you should start the IPython notebook server from the <code>assignment1</code> directory, with the <code>jupyter notebook</code> command. (See the <a href="http://cs231n.github.io/gce-tutorial/" target="_blank" rel="noopener">Google Cloud Tutorial</a> for any additional steps you may need to do for setting this up, if you are working remotely)</p><p>If you are unfamiliar with IPython, you can also refer to our <a href="http://cs231n.github.io/ipython-tutorial" target="_blank" rel="noopener">IPython tutorial</a>.</p><h3 id="Some-Notes"><a href="#Some-Notes" class="headerlink" title="Some Notes"></a>Some Notes</h3><p><strong>NOTE 1:</strong> This year, the <code>assignment1</code> code has been tested to be compatible with python versions <code>2.7</code>, <code>3.5</code>, <code>3.6</code> (it may work with other versions of <code>3.x</code>, but we won’t be officially supporting them). You will need to make sure that during your <code>virtualenv</code> setup that the correct version of <code>python</code> is used. You can confirm your python version by (1) activating your virtualenv and (2) running <code>which python</code>.</p><p><strong>NOTE 2:</strong> If you are working in a virtual environment on OSX, you may <em>potentially</em> encounter errors with matplotlib due to the <a href="http://matplotlib.org/faq/virtualenv_faq.html" target="_blank" rel="noopener">issues described here</a>. In our testing, it seems that this issue is no longer present with the most recent version of matplotlib, but if you do end up running into this issue you may have to use the <code>start_ipython_osx.sh</code> script from the <code>assignment1</code> directory (instead of <code>jupyter notebook</code> above) to launch your IPython notebook server. Note that you may have to modify some variables within the script to match your version of python/installation directory. The script assumes that your virtual environment is named <code>.env</code>.</p><h3 id="Submitting-your-work"><a href="#Submitting-your-work" class="headerlink" title="Submitting your work:"></a>Submitting your work:</h3><p>Whether you work on the assignment locally or using Google Cloud, once you are done working run the <code>collectSubmission.sh</code> script; this will produce a file called <code>assignment1.zip</code>. Please submit this file on <a href="https://canvas.stanford.edu/courses/66461/" target="_blank" rel="noopener">Canvas</a>.</p><h3 id="Q1-k-Nearest-Neighbor-classifier-20-points"><a href="#Q1-k-Nearest-Neighbor-classifier-20-points" class="headerlink" title="Q1: k-Nearest Neighbor classifier (20 points)"></a>Q1: k-Nearest Neighbor classifier (20 points)</h3><p>The IPython Notebook <strong>knn.ipynb</strong> will walk you through implementing the kNN classifier.</p><h3 id="Q2-Training-a-Support-Vector-Machine-25-points"><a href="#Q2-Training-a-Support-Vector-Machine-25-points" class="headerlink" title="Q2: Training a Support Vector Machine (25 points)"></a>Q2: Training a Support Vector Machine (25 points)</h3><p>The IPython Notebook <strong>svm.ipynb</strong> will walk you through implementing the SVM classifier.</p><h3 id="Q3-Implement-a-Softmax-classifier-20-points"><a href="#Q3-Implement-a-Softmax-classifier-20-points" class="headerlink" title="Q3: Implement a Softmax classifier (20 points)"></a>Q3: Implement a Softmax classifier (20 points)</h3><p>The IPython Notebook <strong>softmax.ipynb</strong> will walk you through implementing the Softmax classifier.</p><h3 id="Q4-Two-Layer-Neural-Network-25-points"><a href="#Q4-Two-Layer-Neural-Network-25-points" class="headerlink" title="Q4: Two-Layer Neural Network (25 points)"></a>Q4: Two-Layer Neural Network (25 points)</h3><p>The IPython Notebook <strong>two_layer_net.ipynb</strong> will walk you through the implementation of a two-layer neural network classifier.</p><h3 id="Q5-Higher-Level-Representations-Image-Features-10-points"><a href="#Q5-Higher-Level-Representations-Image-Features-10-points" class="headerlink" title="Q5: Higher Level Representations: Image Features (10 points)"></a>Q5: Higher Level Representations: Image Features (10 points)</h3><p>The IPython Notebook <strong>features.ipynb</strong> will walk you through this exercise, in which you will examine the improvements gained by using higher-level representations as opposed to using raw pixel values.</p><h3 id="Q6-Cool-Bonus-Do-something-extra-10-points"><a href="#Q6-Cool-Bonus-Do-something-extra-10-points" class="headerlink" title="Q6: Cool Bonus: Do something extra! (+10 points)"></a>Q6: Cool Bonus: Do something extra! (+10 points)</h3><p>Implement, investigate or analyze something extra surrounding the topics in this assignment, and using the code you developed. For example, is there some other interesting question we could have asked? Is there any insightful visualization you can plot? Or anything fun to look at? Or maybe you can experiment with a spin on the loss function? If you try out something cool we’ll give you up to 10 extra points and may feature your results in the lecture.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier. The goals of this assignment are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;understand the basic &lt;strong&gt;Image Classification pipeline&lt;/strong&gt; and the data-driven approach (train/predict stages)&lt;/li&gt;
&lt;li&gt;understand the train/val/test &lt;strong&gt;splits&lt;/strong&gt; and the use of validation data for &lt;strong&gt;hyperparameter tuning&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;develop proficiency in writing efficient &lt;strong&gt;vectorized&lt;/strong&gt; code with numpy&lt;/li&gt;
&lt;li&gt;implement and apply a k-Nearest Neighbor (&lt;strong&gt;kNN&lt;/strong&gt;) classifier&lt;/li&gt;
&lt;li&gt;implement and apply a Multiclass Support Vector Machine (&lt;strong&gt;SVM&lt;/strong&gt;) classifier&lt;/li&gt;
&lt;li&gt;implement and apply a &lt;strong&gt;Softmax&lt;/strong&gt; classifier&lt;/li&gt;
&lt;li&gt;implement and apply a &lt;strong&gt;Two layer neural network&lt;/strong&gt; classifier&lt;/li&gt;
&lt;li&gt;understand the differences and tradeoffs between these classifiers&lt;/li&gt;
&lt;li&gt;get a basic understanding of performance improvements from using &lt;strong&gt;higher-level representations&lt;/strong&gt; than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="paper" scheme="http://xiekai.site/categories/paper/"/>
    
    
      <category term="tag1" scheme="http://xiekai.site/tags/tag1/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://xiekai.site/2018/01/31/hello-world/"/>
    <id>http://xiekai.site/2018/01/31/hello-world/</id>
    <published>2018-01-31T00:53:23.954Z</published>
    <updated>2018-02-05T06:18:37.166Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h1 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h1><h4 id="Show-Some-Python-Code"><a href="#Show-Some-Python-Code" class="headerlink" title="Show Some Python Code"></a>Show Some Python Code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">D = np.random.randn(<span class="number">1000</span>,<span class="number">500</span>)</span><br><span class="line">hidden_layer_sizes = [<span class="number">500</span>]*<span class="number">10</span></span><br><span class="line">nonlinearities = [<span class="string">'tanh'</span>]*len(hidden_layer_sizes)</span><br><span class="line"></span><br><span class="line">act = &#123;<span class="string">'relu'</span>:<span class="keyword">lambda</span> x:np.maximum(<span class="number">0</span>,x), <span class="string">'tanh'</span>:<span class="keyword">lambda</span> x:np.tanh(x)&#125;</span><br><span class="line">Hs = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(hidden_layer_sizes)):</span><br><span class="line">    X = D <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> Hs[i<span class="number">-1</span>] <span class="comment"># input at this layer</span></span><br><span class="line">    fan_in = X.shape[<span class="number">1</span>]</span><br><span class="line">    fan_out = hidden_layer_sizes[i]</span><br><span class="line">    W = np.random.randn(fan_in, fan_out) * <span class="number">0.01</span> <span class="comment"># layer init</span></span><br><span class="line"></span><br><span class="line">    H = np.dot(X, W) <span class="comment"># matrix multiply</span></span><br><span class="line">    H = act[nonlinearities[i]](H) <span class="comment"># nonlinearity</span></span><br><span class="line">    Hs[i] = H <span class="comment"># cache result on this layer</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h4 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h4 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h4 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="essay" scheme="http://xiekai.site/categories/essay/"/>
    
    
  </entry>
  
</feed>
