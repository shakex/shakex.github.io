<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> test4 · Shake and Shake's Vision</title><meta name="description" content="test4 - Shake"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://xiekai.site/atom.xml" title="Shake and Shake's Vision"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">博客</a></li><li class="nav-list-item"><a href="/vision/" target="_self" class="nav-list-link">图集</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">我</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">订阅</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">test4</h1><div class="post-info">2018年2月6日</div><div class="post-content"><h1 id="This-is-a-h1-title"><a href="#This-is-a-h1-title" class="headerlink" title="This is a h1 title"></a>This is a h1 title</h1><h2 id="This-is-a-h2-title"><a href="#This-is-a-h2-title" class="headerlink" title="This is a h2 title"></a>This is a h2 title</h2><h3 id="This-is-a-h3-title"><a href="#This-is-a-h3-title" class="headerlink" title="This is a h3 title"></a>This is a h3 title</h3><h4 id="This-is-a-h4-title"><a href="#This-is-a-h4-title" class="headerlink" title="This is a h4 title"></a>This is a h4 title</h4><h5 id="This-is-a-h5-title"><a href="#This-is-a-h5-title" class="headerlink" title="This is a h5 title"></a>This is a h5 title</h5><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><p><strong>Paragraph</strong> In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. Moreover, as we will see later in the course, many other seemingly distinct Computer Vision tasks (such as object detection, segmentation) can be reduced to image classification.</p>
<ul>
<li>A single instance of an object can be oriented in many ways with respect to the camera.</li>
<li>Many objects of interest are not rigid bodies and can be deformed in extreme ways.</li>
<li>The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.</li>
<li>The effects of illumination are drastic on the pixel level.<ul>
<li>Background clutter</li>
<li>Intra-class variation</li>
</ul>
<ol>
<li>Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set.</li>
<li>Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model.</li>
</ol>
</li>
</ul>
<h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><p>You are probably familiar with the concept of a derivative in the scalar case: given a function $f : \mathbb{R} \rightarrow \mathbb{R}$, the derivative of f at a point $x ∈ \mathbb{R}$ is defined as:</p>
<script type="math/tex; mode=display">y(x,w)=w_0+\sum_{j=1}^{M-1}w_j \phi_j(x) \tag{1-1}</script><p>Derivatives are a way to measure change. In the scalar case, the derivative of the function $f$ at the point $x$ tells us how much the function $f$ changes as the input $x$ changes by a small amount $\varepsilon$:</p>
<script type="math/tex; mode=display">y(x,w)=\sum_{j=0}^{M-1}w_j \phi_j(x)=w^T\phi(x) \tag{1-2}</script><p>For ease of notation we will commonly assign a name to the output of $f$, say $y = f(x)$, and write $\frac{\partial{y}}{\partial{x}}$ for the derivative of $y$ with respect to $x$. This $\partial{x}$ notation emphasizes that $\frac{\partial{y}}{\partial{x}}$ is the <em>rate of change</em> between the variables $x$ and $y$; Concretely if $x$ were to change by $\varepsilon$ then $y$ will change by approximately $\varepsilon \frac{\partial{y}}{\partial{x}}$. We can write this relationship as</p>
<script type="math/tex; mode=display">\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}} \tag{1-3}</script><h2 id="中文与公式"><a href="#中文与公式" class="headerlink" title="中文与公式"></a>中文与公式</h2><p>过拟合是指学习时选择的模型所包含的参数过多，导致对已知数据预测得很好，但对未知数据预测很差的现象。即所得的假设函数拥有较低的训练误差，但有较高的泛化误差。导致过拟合的主要影响因素有：模型的复杂度、训练数据的噪声、数据规模。过拟合无法彻底避免，只能减小其风险。</p>
<p>最大似然估计就是用似然函数取到最大值时的参数值作为估计值，似然函数可以写做：</p>
<script type="math/tex; mode=display">L(\theta)=p(X | \theta)=\prod_{i=n}^N p(x_n | \theta) \tag{2-1}</script><p>为防止浮点数下溢与方便数值求解，通常取对数似然：</p>
<script type="math/tex; mode=display">LL(\theta)=\sum_{n=1}^N \ln p(x_n | \theta) \tag{2-2}</script><p>所以，$\hat{\theta}_{ML}$ 表示为（取平均对数似然）：</p>
<script type="math/tex; mode=display">\hat{\theta}_{ML}=\arg\max_\theta \frac{1}{N} \sum_{n=1}^N \ln p(x_n | \theta) \tag{2-3}</script><p>当使用数值方法求解极小值时，通常会将最大化问题转化为最小化问题，即取负对数。</p>
<h2 id="中文文章"><a href="#中文文章" class="headerlink" title="中文文章"></a>中文文章</h2><p>&#160; &#160; &#160; &#160;我舅舅上个世纪（20世纪）末生活在世界上。有件事我们大家都知道：在中国，历史以三十年为极限，我们不可能知道三十年以前的事。我舅舅比我大了三十多岁，所以他的事我就不大知道——更正确的说法是不该知道。他留下了一大堆的笔记、相片，除此之外，我还记得他的样子。他是个肤色黝黑的大个子，年轻时头发很多，老了就秃了。他们那个时候的事情，我们知道的只是：当时烧煤，烧得整个天空乌烟障气，而且大多数人骑车上班。自行车这种体育器械，在当年是一种代步工具，样子和今天的也大不相同，在两个轮子之间有一个三角形的钢管架子，还有一根管子竖在此架子之上。流传到现在的车里有一小部分该管子上面有个车座，另一部分上面什么都没有；此种情形使考古学家大惑不解，有人说后一些车子的座子遗失了，还有人提出了更深刻的解释——当时的人里有一部分是受信任的，可以享受比较好的生活，有座的车就属于他们。另一部分人不受信任，所以必须一刻不停地折磨自己，才能得到活下去的权利，故而这种不带座子的自行车就是他们对肛门、会阴部实施自残自虐的工具。根据我的童年印象，这后一种说法颇为牵强。我还记得人们是怎样骑自行车的。但是我不想和权威争辩——上级现在还信任我，我也不想自讨没趣。</p>
<p>&#160; &#160; &#160; &#160;舅舅是个作家，但是在他生前一部作品也没发表过，这是他不受信任的铁证。因为这个原故，他的作品现在得以出版，并且堆积在书店里无人问津。众所周知，现在和那时大不一样了，我们的社会发生了重大转折，走向了光明。——不管怎么说吧，作为外甥，我该为此大为欢喜，但是书商恐怕会有另一种结论。我舅舅才情如何，自然该由古典文学的研究者来评判，我知道的只是：现在纸张书籍根本不受欢迎，受欢迎的是电子书籍，还该有多媒体插图。所以书商真的要让我舅舅重见天日的话，就该多投点资，把我舅舅的书编得像点样子。现在他们又找到我，让我给他老人家写一本传记，其中必须包括他骑那种没有座的自行车，并且要考据出他得了痔疮，甚至前列腺癌。但是根据我掌握的材料，我舅舅患有各种疾病，包括关节炎、心脏病，但上述器官没有一种长在肛门附近，是那种残酷的车辆导致的。他死于一次电梯事故，一下子就被压扁了，这是个让人羡慕的死法，明显地好于死于前列腺癌。这就使我很为难了。我本人是学历史的，历史是文科；所以我知道文科的导向原则——这就是说，一切形成文字的东西，都应当导向一个对我们有利的结论。我舅舅已经死了，让他死于痔疮、前列腺癌，对我们有利，就让他这样死，本无不可。但是这样一来，我就不知死在电梯里的那个老头子是谁了。他死时我已经二十岁，记得事。当时他坐电梯要到十四楼，却到了地下室，而且变得肢体残缺。有人说，那电梯是废品，每天都坏，还说管房子的收了包工头的回扣。这样说不够“导向”——这样他就是死于某个人的贪心、而不是死于制度的弊病了。必须另给他个死法。这个问题我能解决，因为我在中文系修了好几年的写作课，专门研究如何臭编的问题。</p>
<h1 id="Going-deeper-with-convolutions"><a href="#Going-deeper-with-convolutions" class="headerlink" title="Going deeper with convolutions"></a>Going deeper with convolutions</h1><p><strong>Christian Szegedy</strong> et al.<br><em>Published on arXiv, 2014</em> </p>
<p><strong>[Abstract]</strong> We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>In the last three years, mainly due to the advances of deep learning, more concretely convolutional networks [10], the quality of image recognition and object detection has been progressing at a dra- matic pace. One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detec- tion purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly more accurate. The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6].</p>
<p>Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms – especially their power and memory use – gains importance. It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.</p>
<p>In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al [12] in conjunction with the famous “we need to go deeper” internet meme [1]. In our case, the word “deep” is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the “Inception module” and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of [12] while taking inspiration and guidance from the theoretical work by Arora et al [2]. The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, on which it significantly outperforms the current state of the art.</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p>Starting with LeNet-5 [10], convolutional neural networks (CNN) have typically had a standard structure – stacked convolutional layers (optionally followed by contrast normalization and max- pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge [9, 21]. For larger datasets such as Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14], while using dropout [7] to address the problem of overfitting.</p>
<p>Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as [9] has also been successfully employed for localization [9, 14], object detection [6, 14, 18, 5] and human pose estimation [19]. Inspired by a neuroscience model of the primate visual cortex, Serre et al. [15] use a series of fixed Gabor filters of different sizes in order to handle multiple scales, similarly to the Inception model. However, contrary to the fixed 2-layer deep model of [15], all filters in the Inception model are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model.</p>
<p>Network-in-Network is an approach proposed by Lin et al. [12] in order to increase the representa- tional power of neural networks. When applied to convolutional layers, the method could be viewed as additional 1 × 1 convolutional layers followed typically by the rectified linear activation [9]. This enables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our architecture. However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks. This allows for not just increasing the depth, but also the width of our networks without significant performance penalty.</p>
<p>The current leading approach for object detection is the Regions with Convolutional Neural Net- works (R-CNN) proposed by Girshick et al. [6]. R-CNN decomposes the overall detection problem into two subproblems: to first utilize low-level cues such as color and superpixel consistency for potential object proposals in a category-agnostic fashion, and to then use CNN classifiers to identify object categories at those locations. Such a two stage approach leverages the accuracy of bound- ing box segmentation with low-level cues, as well as the highly powerful classification power of state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have ex- plored enhancements in both stages, such as multi-box [5] prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.</p>
<h2 id="3-Motivation-and-High-Level-Considerations"><a href="#3-Motivation-and-High-Level-Considerations" class="headerlink" title="3 Motivation and High Level Considerations"></a>3 Motivation and High Level Considerations</h2><p>The most straightforward way of improving the performance of deep neural networks is by increas- ing their size. This includes both increasing the depth – the number of levels – of the network and its width: the number of units at each level. This is as an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data. However this simple solution comes with two major drawbacks.</p>
<img src="http://cs231n.github.io/assets/trainset.jpg">
<blockquote>
<p>Figure 1: An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category.</p>
</blockquote>
<p>Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited. This can become a major bottleneck, since the creation of high quality training sets can be tricky and expensive, especially if expert human raters are necessary to distinguish between fine-grained visual categories like those in ImageNet (even in the 1000-class ILSVRC subset) as demonstrated by <code>Figure 1</code>.</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></span><br><span class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    self.Xtr = X</span><br><span class="line">    self.ytr = y</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over all test rows</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># find the nearest training image to the i'th test image</span></span><br><span class="line">      <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">      min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE HTML&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"content-type"</span> <span class="attr">content</span>=<span class="string">"text/html;charset=utf-8;"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"X-UA-Compatible"</span> <span class="attr">content</span>=<span class="string">"IE=edge,chrome=1"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"robots"</span> <span class="attr">content</span>=<span class="string">"all"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"robots"</span> <span class="attr">content</span>=<span class="string">"index,follow"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"https://qzone.qq.com/gy/404/style/404style.css"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/plain"</span> <span class="attr">src</span>=<span class="string">"http://www.qq.com/404/search_children.js"</span></span></span><br><span class="line"><span class="tag">          <span class="attr">charset</span>=<span class="string">"utf-8"</span> <span class="attr">homePageUrl</span>=<span class="string">"/"</span></span></span><br><span class="line"><span class="tag">          <span class="attr">homePageName</span>=<span class="string">"回到我的主页"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">  </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://qzone.qq.com/gy/404/data.js"</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://qzone.qq.com/gy/404/page.js"</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="Media"><a href="#Media" class="headerlink" title="Media"></a>Media</h2></div></article></div></main><footer><div class="paginator"><a href="/2018/02/05/test3/" class="next">下一篇</a></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
owner: 'shakex',
repo: 'shakex.github.io',
oauth: {
client_id: 'd7dc2b0497fcd0df69cf',
client_secret: '6651a6c77a34ede7e58dfafef3defa7bdfedc0d0',
},
})
gitment.render('container')</script><div class="copyright"><p>© 2018 <a href="http://xiekai.site">Shake</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and modified from <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
showProcessingMessages: false, //关闭js加载过程信息
messageStyle: "none", //不显示信息
extensions: ["tex2jax.js"],
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
inlineMath:  [ ["$", "$"] ], //行内公式选择$
displayMath: [ ["$$","$$"] ], //段内公式选择$$
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a'], //避开某些标签
ignoreClass:"comment-content" //避开含该Class的标签
},
"HTML-CSS": {
availableFonts: ["STIX","TeX"], //可选字体
showMathMenu: false //关闭右击菜单显示
}
});
MathJax.Hub.Queue(["Typeset",MathJax.Hub]);</script><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link href="http://fonts.font.im/css?family=Amatic+SC:700" rel="stylesheet" type="text/css"><link href='http://fonts.font.im/css?family=Merriweather' rel='stylesheet' type='text/css'></body></html>