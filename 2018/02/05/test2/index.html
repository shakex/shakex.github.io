<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> A Review of ECNU PRML Lectures (test) · Shake and Shake's Vision</title><meta name="description" content="a test page"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://xiekai.site/atom.xml" title="Shake and Shake's Vision"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">博客</a></li><li class="nav-list-item"><a href="/vision/" target="_self" class="nav-list-link">图集</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">我</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">订阅</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">A Review of ECNU PRML Lectures (test)</h1><div class="post-info">2018年2月5日</div><div class="post-content"><h2 id="Lecture-1-Introduction-amp-generic-techniques"><a href="#Lecture-1-Introduction-amp-generic-techniques" class="headerlink" title="Lecture 1: Introduction &amp; generic techniques"></a>Lecture 1: Introduction &amp; generic techniques</h2><h3 id="过拟合-Overfitting-与规范化-Regularization"><a href="#过拟合-Overfitting-与规范化-Regularization" class="headerlink" title="过拟合(Overfitting)与规范化(Regularization)"></a>过拟合(Overfitting)与规范化(Regularization)</h3><ul>
<li><p>过拟合<br>过拟合是指学习时选择的模型所包含的参数过多，导致对已知数据预测得很好，但对未知数据预测很差的现象。即所得的假设函数拥有较低的训练误差，但有较高的泛化误差。导致过拟合的主要影响因素有：模型的复杂度、训练数据的噪声、数据规模。过拟合无法彻底避免，只能减小其风险。</p>
</li>
<li><p>规范化<br>规范化（正则化）是一种控制过拟合现象的技术。通过给误差函数增加一个惩罚项，使得系数不会达到很大的值。这种惩罚项最简单的形式是采用所有系数的平方和的形式。<br>一般形式：</p>
<script type="math/tex; mode=display">\arg\min \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)</script><p>其中，第一项为经验风险，第二项为规范化项，$\lambda$调整两者之间的关系。</p>
</li>
</ul>
<h3 id="参数估计方法"><a href="#参数估计方法" class="headerlink" title="参数估计方法"></a>参数估计方法</h3><p>频率学派：就事件本身建模。认为参数固定，样本随机。只要样本分布确定了，参数就是个固定的值。e.g. MLE<br>贝叶斯派：参数随机，样本固定。参数服从先验分布，通过不断获取新的样本，去修正参数（即形成后验分布）e.g. MAP</p>
<ul>
<li>贝叶斯公式<script type="math/tex; mode=display">p(\theta|X)=\frac{p(X|\theta)p(\theta)}{p(X)}</script></li>
</ul>
<p>给定数据集：$X=(x_1,x_2,…,x_N)^T$</p>
<h4 id="最大似然估计（MLE）"><a href="#最大似然估计（MLE）" class="headerlink" title="最大似然估计（MLE）"></a>最大似然估计（MLE）</h4><p>最大似然估计就是用似然函数取到最大值时的参数值作为估计值，似然函数可以写做：</p>
<script type="math/tex; mode=display">L(\theta)=p(X | \theta)=\prod_{i=n}^N p(x_n | \theta)</script><p>为防止浮点数下溢与方便数值求解，通常取对数似然：</p>
<script type="math/tex; mode=display">LL(\theta)=\sum_{n=1}^N \ln p(x_n | \theta)</script><p>所以，$\hat{\theta}_{ML}$表示为（取平均对数似然）：</p>
<script type="math/tex; mode=display">\hat{\theta}_{ML}=\arg\max_\theta \frac{1}{N} \sum_{n=1}^N \ln p(x_n | \theta)</script><p>当使用数值方法求解极小值时，通常会将最大化问题转化为最小化问题，即取负对数。</p>
<h4 id="最大参数后验（MAP）"><a href="#最大参数后验（MAP）" class="headerlink" title="最大参数后验（MAP）"></a>最大参数后验（MAP）</h4><p>最大后验估计是根据经验数据获得对难以观察的量的点估计。最大后验加入了要估计量的先验分布。</p>
<script type="math/tex; mode=display">posterior \propto likelihood \times prior</script><p>取对数则有：</p>
<script type="math/tex; mode=display">\ln\{posterior\} \propto \ln\{likelihood\} + \ln\{prior\}</script><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><h2 id="Lecture-2-Gaussian-distribution-amp-related-techniques"><a href="#Lecture-2-Gaussian-distribution-amp-related-techniques" class="headerlink" title="Lecture 2: Gaussian distribution &amp; related techniques"></a>Lecture 2: Gaussian distribution &amp; related techniques</h2><h3 id="高斯分布的性质"><a href="#高斯分布的性质" class="headerlink" title="高斯分布的性质"></a>高斯分布的性质</h3><ul>
<li>一元变量$x$的情形<script type="math/tex; mode=display">\mathcal N(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\{-\frac{1}{2\sigma^2}(x-\mu)^2\}</script>其中$\mu$是均值，$\sigma^2$是方差。<ul>
<li>$\mathbb E[x]=\mu$</li>
<li>$var[x]=\sigma^2$</li>
</ul>
</li>
<li>多元情况<script type="math/tex; mode=display">\mathcal N(x|\mu,\Sigma)=\frac{1}{(2\pi)^\frac{D}{2}}\frac{1}{|\Sigma|^\frac{1}{2}}\exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}</script>其中，$\mu$是一个D维均值向量，$\Sigma$是一个$D\times D$的协方差矩阵，$|\Sigma|$是$\Sigma$的行列式<ul>
<li>$\mathbb E[\bf{x}]=\bf{\mu}$</li>
<li>$cov[\bf{x}]=\bf{\Sigma}$</li>
</ul>
</li>
<li>中心极限定理：设从均值为$\mu$，方差为$\sigma^2$（有限）的任意一个总体中抽取样本量为$n$的样本；当$n$充分大时，样本均值的抽样分布近似服从$\mu$，方差为$\frac{\sigma^2}{n}$的正态分布。</li>
</ul>
<h3 id="Woodbury矩阵求逆"><a href="#Woodbury矩阵求逆" class="headerlink" title="Woodbury矩阵求逆"></a>Woodbury矩阵求逆</h3><script type="math/tex; mode=display">(A+BCD)^{-1}=A^{-1}-A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}</script><h2 id="Lecture-3-Linear-models-for-regression"><a href="#Lecture-3-Linear-models-for-regression" class="headerlink" title="Lecture 3: Linear models for regression"></a>Lecture 3: Linear models for regression</h2><p>回归问题的目标是在给定$D$维输入（input）变量$x$的情况下，预测一个或者多个连续目标（target）变量$t$的值。<br>给定一个由$N$个观测值$\{x_n\}$组成的数据集，其中$n=1,…,N$，以及对应的目标值{t_n}，我们的目标是预测对于给定新的$x$值的情况下，$t$的值。</p>
<ul>
<li>方法：<ol>
<li>建立一个适当的函数$y(x)$，对于新$x$，直接给出对应的$t$值。</li>
<li>对预测分布$p(t | x)$建模。（等同于最小化一个恰当选择的损失函数的期望值）</li>
</ol>
</li>
</ul>
<h3 id="线性回归模型（线性基函数模型）"><a href="#线性回归模型（线性基函数模型）" class="headerlink" title="线性回归模型（线性基函数模型）"></a>线性回归模型（线性基函数模型）</h3><ul>
<li>线性回归模型：表示模型是关于参数的线性函数</li>
<li><p>线性基函数模型</p>
<script type="math/tex; mode=display">y(x,w)=w_0+\sum_{j=1}^{M-1}w_j \phi_j(x)</script><p>其中，$x=(x1,…,x_D)^T, w=(w0,…,w_D)^T, \phi_j(x)$为基函数（basis function），参数总数为$M$。<br>方便起见，定义$\phi_0(x)=1$：</p>
<script type="math/tex; mode=display">y(x,w)=\sum_{j=0}^{M-1}w_j \phi_j(x)=w^T\phi(x)</script><p>其中，$\phi=(\phi_0,…,\phi_{M-1})^T$</p>
</li>
<li><p>通过使用非线性基函数，能够让函数$y(x,w)$成为输入向量$x$的一个非线性函数。通常选择如下基函数：</p>
<ul>
<li>幂指数<script type="math/tex; mode=display">\phi_j(x)=x^j</script></li>
<li>高斯基函数<script type="math/tex; mode=display">\phi_j(x)=exp\{-\frac{(x-\mu_j)^2}{2s^2}\}</script>$\mu_j$控制了基函数在输入空间中的位置，参数$s$控制了基函数的空间大小</li>
<li>sigmoid基函数<script type="math/tex; mode=display">\phi_j(x)=\sigma(\frac{x-\mu_j}{s})</script>其中$\sigma(a)=\frac{1}{1+e^{-a}}$为logistic sigmoid</li>
<li>傅立叶基函数</li>
</ul>
</li>
</ul>
<h3 id="最小二乘与规范化最小二乘"><a href="#最小二乘与规范化最小二乘" class="headerlink" title="最小二乘与规范化最小二乘"></a>最小二乘与规范化最小二乘</h3><script type="math/tex; mode=display">E_D(w)=\frac{1}{2} \sum_{n=1}^N\{t_n-w^T \phi(x_n)\}^2</script><h4 id="规范化最小二乘"><a href="#规范化最小二乘" class="headerlink" title="规范化最小二乘"></a>规范化最小二乘</h4><p>总的误差函数表达为：$E_D(w)+\lambda E_W(w)$</p>
<ul>
<li><p>正则化项$E_W(w)$的选择</p>
<ul>
<li>权向量各个元素的平方和<script type="math/tex; mode=display">E_W(w)=\frac{1}{2}w^Tw</script></li>
<li><p>更一般化</p>
<script type="math/tex; mode=display">E_W(w)=\frac{1}{2}\sum_{j=1}^M |w_j|^q</script></li>
<li><p>$q=1$的情形称为套索（lasso），$q=2$对应于二次规范化项。</p>
</li>
</ul>
</li>
</ul>
<h2 id="Lecture-4-Essential-numerical-mathematics-amp-vector-calculus"><a href="#Lecture-4-Essential-numerical-mathematics-amp-vector-calculus" class="headerlink" title="Lecture 4: Essential numerical mathematics &amp; vector calculus"></a>Lecture 4: Essential numerical mathematics &amp; vector calculus</h2><h3 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h3><h3 id="共轭梯度算法"><a href="#共轭梯度算法" class="headerlink" title="共轭梯度算法"></a>共轭梯度算法</h3><h3 id="向量微积分"><a href="#向量微积分" class="headerlink" title="向量微积分"></a>向量微积分</h3><h2 id="Lecture-5-Linear-models-for-classification"><a href="#Lecture-5-Linear-models-for-classification" class="headerlink" title="Lecture 5: Linear models for classification"></a>Lecture 5: Linear models for classification</h2><p>分类的目标是将输入变量$x$分到K个离散的类别$C_k$中的某一类。</p>
<ul>
<li>分类线性模型：决策面是$x$的线性函数<h3 id="判别函数、概率产生式模型、概率判别式模型"><a href="#判别函数、概率产生式模型、概率判别式模型" class="headerlink" title="判别函数、概率产生式模型、概率判别式模型"></a>判别函数、概率产生式模型、概率判别式模型</h3>对于分类问题：可分为推断（inference）和决策（decision）两个阶段。推断阶段，使用训练数据学习$p(C_k|x)$的模型；决策阶段，使用后验概率来进行最优的分类。也可以同时解决两个问题，即简单地学习一个函数（判别函数，discriminant function），将输入$x$直接映射为决策。</li>
<li>三种方法：</li>
</ul>
<ol>
<li>判别函数（discriminative function）<br>把每个输入$x$直接映射为类别标签（不接触后验概率）</li>
<li>概率判别式模型（discriminative model）<br>直接对后验概率分布$p(C_k | x)$建模，例如把条件概率分布表示为参数模型，然后使用训练集来最优化参数。</li>
<li>概率生成式模型（generative model）<br>对类条件概率密度$p(x | C_k)$以及类的先验概率分布$p(C_k)$建模，然后我们使用贝叶斯定理计算后验概率分布$p(C_k | x)$</li>
</ol>
<h3 id="线性判别函数"><a href="#线性判别函数" class="headerlink" title="线性判别函数"></a>线性判别函数</h3><ul>
<li><p>二分类</p>
<script type="math/tex; mode=display">y(x)=w^Tx+w_0</script><p>对于一个输入向量$x$，如果$y(x) \leq 0$，则分到$C_1$，否则分到$C_2$。</p>
</li>
<li><p>K分类<br>K分类由K个线性判别函数组成，形式为：</p>
<script type="math/tex; mode=display">y_k(x)=w_k^Tx+w_{k0}</script><p>对于一个输入向量$x$，如果对于所有的$j \neq k$都有$y_k(x)&gt;y_j(x)$，那么就把它分到$C_k$。</p>
</li>
<li><p>学习参数的方法</p>
<ul>
<li><p>最小平方法：使模型的预测尽可能与目标值接近。缺点：对于离群点缺乏鲁棒性</p>
</li>
<li><p>Fisher线性判别</p>
<p>基本思想：从维度降低的角度看线性分类模型。目标是使输出空间的类别有最大的区分度。通过调整权向量，使得类内的投影点尽可能接近，类间的投影点尽可能远离。<br>二分类情况：$C_1$类的点有$N_1$个，$C_2$类的点有$N_2$个<br>两类的均值向量为：$m_1=\frac{1}{N_1}\sum_{n \in C_1}x_n, m_2=\frac{1}{N_2}\sum_{n \in C_2}x_n$</p>
</li>
</ul>
</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2018/02/05/test3/" class="prev">上一篇</a><a href="/2018/01/31/test/" class="next">下一篇</a></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
owner: 'shakex',
repo: 'shakex.github.io',
oauth: {
client_id: 'd7dc2b0497fcd0df69cf',
client_secret: '6651a6c77a34ede7e58dfafef3defa7bdfedc0d0',
},
})
gitment.render('container')</script><div class="copyright"><p>© 2018 <a href="http://xiekai.site">Shake</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and modified from <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
showProcessingMessages: false, //关闭js加载过程信息
messageStyle: "none", //不显示信息
extensions: ["tex2jax.js"],
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
inlineMath:  [ ["$", "$"] ], //行内公式选择$
displayMath: [ ["$$","$$"] ], //段内公式选择$$
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a'], //避开某些标签
ignoreClass:"comment-content" //避开含该Class的标签
},
"HTML-CSS": {
availableFonts: ["STIX","TeX"], //可选字体
showMathMenu: false //关闭右击菜单显示
}
});
MathJax.Hub.Queue(["Typeset",MathJax.Hub]);</script><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link href="http://fonts.font.im/css?family=Amatic+SC:700" rel="stylesheet" type="text/css"><link href='http://fonts.font.im/css?family=Merriweather' rel='stylesheet' type='text/css'></body></html>